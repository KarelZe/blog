@article{aasExplainingIndividualPredictions2021,
  title = {Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values},
  author = {Aas, Kjersti and Jullum, Martin and L{\o}land, Anders},
  year = 2021,
  journal = {Artificial Intelligence},
  volume = {298},
  pages = {1--24},
  doi = {10.1016/j.artint.2021.103502}
}

@misc{abagyanOneTokenizerRule2025,
  title = {One {{Tokenizer To Rule Them All}}: {{Emergent Language Plasticity}} via {{Multilingual Tokenizers}}},
  shorttitle = {One {{Tokenizer To Rule Them All}}},
  author = {Abagyan, Diana and Salamanca, Alejandro R. and {Cruz-Salinas}, Andres Felipe and Cao, Kris and Lin, Hangyu and Locatelli, Acyr and Fadaee, Marzieh and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  year = 2025,
  month = jun,
  number = {arXiv:2506.10766},
  eprint = {2506.10766},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.10766},
  urldate = {2025-06-13},
  abstract = {Pretraining massively multilingual Large Language Models (LLMs) for many languages at once is challenging due to limited model capacity, scarce high-quality data, and compute constraints. Moreover, the lack of language coverage of the tokenizer makes it harder to address the gap for new languages purely at the post-training stage. In this work, we study what relatively cheap interventions early on in training improve ``language plasticity'', or adaptation capabilities of the model post-training to new languages. We focus on tokenizer design and propose using a universal tokenizer that is trained for more languages than the primary pretraining languages to enable efficient adaptation in expanding language coverage after pretraining. Our systematic experiments across diverse groups of languages and different training strategies show that a universal tokenizer enables significantly higher language adaptation, with up to 20.2\% increase in win rates compared to tokenizers specific to pretraining languages. Furthermore, a universal tokenizer also leads to better plasticity towards languages that are completely unseen in the tokenizer and pretraining, by up to 5\% win rate gain. We achieve this adaptation to an expanded set of languages with minimal compromise in performance on the majority of languages included in pretraining.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@misc{abdinPhi3TechnicalReport2024,
  title = {Phi-3 {{Technical Report}}: {{A Highly Capable Language Model Locally}} on {{Your Phone}}},
  shorttitle = {Phi-3 {{Technical Report}}},
  author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Martin and Cai, Qin and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng, Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao, Mei and Gao, Min and Garg, Amit and Giorno, Allie Del and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Hu, Wenxiang and Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and Kim, Young Jin and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu, Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh, Ali and Majercak, David and Mazzola, Matt and Mendes, Caio C{\'e}sar Teodoro and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and {Perez-Becker}, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Ren, Liliang and de Rosa, Gustavo and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang, Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin and Wang, Yu and Ward, Rachel and Wen, Wen and Witte, Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  year = 2024,
  month = aug,
  number = {arXiv:2404.14219},
  eprint = {2404.14219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.14219},
  urldate = {2025-12-11},
  abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\%, 78\% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language Model Locally on Your Phone.pdf}
}

@misc{abdinPhi4TechnicalReport2024,
  title = {Phi-4 {{Technical Report}}},
  author = {Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J. and Javaheripi, Mojan and Kauffmann, Piero and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liu, Weishung and Mendes, Caio C. T. and Nguyen, Anh and Price, Eric and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Wang, Xin and Ward, Rachel and Wu, Yue and Yu, Dingli and Zhang, Cyril and Zhang, Yi},
  year = 2024,
  month = dec,
  number = {arXiv:2412.08905},
  eprint = {2412.08905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08905},
  urldate = {2024-12-13},
  abstract = {We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Abdin et al. - 2024 - Phi-4 Technical Report.pdf}
}

@incollection{abeDeepLearningForecasting2018,
  title = {Deep Learning for Forecasting Stock Returns in the Cross-Section},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Abe, Masaya and Nakayama, Hideki},
  editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  year = 2018,
  volume = {10937},
  pages = {273--284},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93034-3\_22},
  urldate = {2021-10-26}
}

@inproceedings{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying Attention Flow in Transformers},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Abnar, Samira and Zuidema, Willem},
  year = 2020,
  pages = {4190--4197},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.385}
}

@misc{adaloglouHowPositionalEmbeddings2021,
  title = {How Positional Embeddings Work in Self-Attention (Code in Pytorch)},
  author = {Adaloglou, Nikolas},
  year = 2021,
  urldate = {2021-12-16}
}

@misc{agarapImplementingAutoencoderPyTorch2020,
  title = {Implementing an Autoencoder in {{PyTorch}}},
  author = {Agarap, Abien Fred},
  year = 2020,
  urldate = {2021-11-03}
}

@book{aggarwalRecommenderSystems2016,
  title = {Recommender Systems},
  author = {Aggarwal, Charu C.},
  year = 2016,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-29659-3},
  urldate = {2021-04-20}
}

@article{Aït-Sahalia_2009,
  title = {Estimating the Degree of Activity of Jumps in High Frequency Data},
  author = {{A{\"i}t-Sahalia}, Yacine and Jacod, Jean},
  year = 2009,
  journal = {Annals of Statistics},
  doi = {10.1214/08-aos640},
  mag_id = {3106171293},
  pmcid = {null},
  pmid = {null},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Aït-Sahalia and Jacod - 2009 - Estimating the degree of activity of jumps in high frequency data.pdf}
}

@article{Aitken_1996,
  title = {The Accuracy of the Tick Test : Evidence from the Australian Stock Exchange},
  author = {Aitken, Michael J. and Frino, Alex},
  year = 1996,
  journal = {Journal of Banking and Finance},
  doi = {10.1016/s0378-4266(96)00008-8},
  mag_id = {2015189325},
  pmcid = {null},
  pmid = {null}
}

@article{aitkenIntradayAnalysisProbability1995,
  title = {An Intraday Analysis of the Probability of Trading on the Asx at the Asking Price},
  author = {Aitken, Michael and Kua, Amaryllis and Brown, Philip and Watter, Terry and Y. Izan, H.},
  year = 1995,
  journal = {Australian Journal of Management},
  volume = {20},
  number = {2},
  pages = {115--154},
  doi = {10.1177/031289629502000202},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Aitken et al. - 1995 - An intraday analysis of the probability of trading on the asx at the asking price.pdf}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: A next-Generation Hyperparameter Optimization Framework},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = 2019,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {Association for Computing Machinery},
  address = {Anchorage, AK, USA},
  doi = {10.1145/3292500.3330701},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Akiba et al. - 2019 - Optuna a next-generation hyperparameter optimization framework.epub}
}

@article{aktasTradeClassificationAccuracy2014,
  title = {Trade Classification Accuracy for the Bist},
  author = {Aktas, Osman Ulas and Kryzanowski, Lawrence},
  year = 2014,
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {33},
  pages = {259--282},
  doi = {10.1016/j.intfin.2014.08.003}
}

@misc{alaaHowFaithfulYour2022,
  title = {How {{Faithful}} Is Your {{Synthetic Data}}? {{Sample-level Metrics}} for {{Evaluating}} and {{Auditing Generative Models}}},
  shorttitle = {How {{Faithful}} Is Your {{Synthetic Data}}?},
  author = {Alaa, Ahmed M. and van Breugel, Boris and Saveliev, Evgeny and van der Schaar, Mihaela},
  year = 2022,
  month = jul,
  number = {arXiv:2102.08921},
  eprint = {2102.08921},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.08921},
  urldate = {2025-04-04},
  abstract = {Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the image synthesis application, exhibit a limited capacity for diagnosing modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional metric, ({$\alpha$}-Precision, {$\beta$}-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a wide variety of application domains. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional dimension for model performance that quantifies the extent to which a model copies training data---a crucial performance indicator when modeling sensitive and private data. The three metric components are interpretable probabilistic quantities, and can be estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Alaa et al. - 2022 - How Faithful is your Synthetic Data Sample-level Metrics for Evaluating and Auditing Generative Mod.pdf}
}

@book{alammarHandsonLargeLanguage2024,
  title = {Hands-on Large Language Models: Language Understanding and Generation},
  shorttitle = {Hands-on Large Language Models},
  author = {Alammar, Jay and Grootendorst, Maarten},
  year = 2024,
  edition = {1st edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham},
  abstract = {Intro -- Copyright -- Table of Contents -- Preface -- An Intuition-First Philosophy -- Prerequisites -- Book Structure -- Part I: Understanding Language Models -- Part II: Using Pretrained Language Models -- Part III: Training and Fine-Tuning Language Models -- Hardware and Software Requirements -- API Keys -- Conventions Used in This Book -- Using Code Examples -- O'Reilly Online Learning -- How to Contact Us -- Acknowledgments -- Part I. Understanding Language Models -- Chapter 1. An Introduction to Large Language Models -- What Is Language AI? -- A Recent History of Language AI -- Representing Language as a Bag-of-Words -- Better Representations with Dense Vector Embeddings -- Types of Embeddings -- Encoding and Decoding Context with Attention -- Attention Is All You Need -- Representation Models: Encoder-Only Models -- Generative Models: Decoder-Only Models -- The Year of Generative AI -- The Moving Definition of a "Large Language Model" -- The Training Paradigm of Large Language Models -- Large Language Model Applications: What Makes Them So Useful? -- Responsible LLM Development and Usage -- Limited Resources Are All You Need -- Interfacing with Large Language Models -- Proprietary, Private Models -- Open Models -- Open Source Frameworks -- Generating Your First Text -- Summary -- Chapter 2. Tokens and Embeddings -- LLM Tokenization -- How Tokenizers Prepare the Inputs to the Language Model -- Downloading and Running an LLM -- How Does the Tokenizer Break Down Text? -- Word Versus Subword Versus Character Versus Byte Tokens -- Comparing Trained LLM Tokenizers -- Tokenizer Properties -- Token Embeddings -- A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer -- Creating Contextualized Word Embeddings with Language Models -- Text Embeddings (for Sentences and Whole Documents) -- Word Embeddings Beyond LLMs},
  isbn = {978-1-0981-5096-9 978-1-0981-5093-8},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Alammar and Grootendorst - 2024 - Hands-on large language models language understanding and generation 1.epub;/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Alammar and Grootendorst - 2024 - Hands-on large language models language understanding and generation.epub}
}

@article{amel-zadehMachineLearningBasedFinancial2020,
  title = {Machine Learning-Based Financial Statement Analysis},
  author = {{Amel-Zadeh}, Amir and Calliess, Jan-Peter and Kaiser, Daniel and Roberts, Stephen},
  year = 2020,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3520684},
  urldate = {2021-11-03}
}

@misc{aminiSelfTrainingSurvey2023,
  title = {Self-Training: A Survey},
  author = {Amini, Massih-Reza and Feofanov, Vasilii and Pauletto, Loic and Devijver, Emilie and Maximov, Yury},
  year = 2023,
  eprint = {2202.12040},
  archiveprefix = {arXiv}
}

@article{anandStealthTradingOptions2007,
  title = {Stealth {{Trading}} in {{Options Markets}}},
  author = {Anand, Amber and Chakravarty, Sugato},
  year = 2007,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {42},
  number = {1},
  pages = {167--187},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0022109000002234},
  urldate = {2023-06-26},
  jstor = {27647290}
}

@article{antoniouLognormalDistributionStock2004,
  title = {On the Log-Normal Distribution of Stock Market Data},
  author = {Antoniou, I and Ivanov, Vi.V and Ivanov, Va.V and Zrelov, P.V},
  year = 2004,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {331},
  number = {3-4},
  pages = {617--638},
  doi = {10.1016/j.physa.2003.09.034},
  urldate = {2022-12-03}
}

@misc{arikTabnetAttentiveInterpretable2020,
  title = {{{TabNet}}: Attentive Interpretable Tabular Learning},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = 2020,
  eprint = {1908.07442},
  archiveprefix = {arXiv}
}

@misc{arpitWhyRegularizedAutoEncoders2016,
  title = {Why Regularized Auto-Encoders Learn Sparse Representation?},
  author = {Arpit, Devansh and Zhou, Yingbo and Ngo, Hung and Govindaraju, Venu},
  year = 2016,
  eprint = {1505.05561},
  urldate = {2021-11-15},
  archiveprefix = {arXiv}
}

@article{Asquith_2010,
  title = {Short Sales and Trade Classification Algorithms},
  author = {Asquith, Paul and Oman, Rebecca and Safaya, Christopher},
  year = 2010,
  journal = {Journal of Financial Markets},
  doi = {10.2139/ssrn.951420},
  mag_id = {2052362834},
  pmcid = {null},
  pmid = {null}
}

@article{auGroupedFeatureImportance2022,
  title = {Grouped Feature Importance and Combined Features Effect Plot},
  author = {Au, Quay and Herbinger, Julia and Stachl, Clemens and Bischl, Bernd and Casalicchio, Giuseppe},
  year = 2022,
  journal = {Data Mining and Knowledge Discovery},
  volume = {36},
  number = {4},
  pages = {1401--1450},
  doi = {10.1007/s10618-022-00840-5},
  urldate = {2023-05-22}
}

@article{averyRecommenderSystemsEvaluating1997,
  title = {Recommender Systems for Evaluating Computer Messages},
  author = {Avery, Christopher and Zeckhauser, Richard},
  year = 1997,
  journal = {Communications of the ACM},
  volume = {40},
  number = {3},
  pages = {88--89},
  doi = {10.1145/245108.245127},
  urldate = {2021-03-21}
}

@article{badaroTransformersTabularData,
  title = {Transformers for Tabular Data Representation: A Survey of Models and Applications},
  author = {Badaro, Gilbert and Saeed, Mohammed and Papotti, Paolo}
}

@inproceedings{bahdanauNeuralMachineTranslation2016,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = 2015,
  address = {San Diego, CA, USA}
}

@inproceedings{bahriSCARFSelfsupervisedContrastive2022,
  title = {{{SCARF}}: Self-Supervised Contrastive Learning Using Random Feature Corruption},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Learning Representations}}},
  author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  year = 2022,
  address = {Online}
}

@misc{baiQwen3VLTechnicalReport2025,
  title = {Qwen3-{{VL Technical Report}}},
  author = {Bai, Shuai and Cai, Yuxuan and Chen, Ruizhe and Chen, Keqin and Chen, Xionghui and Cheng, Zesen and Deng, Lianghao and Ding, Wei and Gao, Chang and Ge, Chunjiang and Ge, Wenbin and Guo, Zhifang and Huang, Qidong and Huang, Jie and Huang, Fei and Hui, Binyuan and Jiang, Shutong and Li, Zhaohai and Li, Mingsheng and Li, Mei and Li, Kaixin and Lin, Zicheng and Lin, Junyang and Liu, Xuejing and Liu, Jiawei and Liu, Chenglong and Liu, Yang and Liu, Dayiheng and Liu, Shixuan and Lu, Dunjie and Luo, Ruilin and Lv, Chenxu and Men, Rui and Meng, Lingchen and Ren, Xuancheng and Ren, Xingzhang and Song, Sibo and Sun, Yuchong and Tang, Jun and Tu, Jianhong and Wan, Jianqiang and Wang, Peng and Wang, Pengfei and Wang, Qiuyue and Wang, Yuxuan and Xie, Tianbao and Xu, Yiheng and Xu, Haiyang and Xu, Jin and Yang, Zhibo and Yang, Mingkun and Yang, Jianxin and Yang, An and Yu, Bowen and Zhang, Fei and Zhang, Hang and Zhang, Xi and Zheng, Bo and Zhong, Humen and Zhou, Jingren and Zhou, Fan and Zhou, Jing and Zhu, Yuanzhi and Zhu, Ke},
  year = 2025,
  month = nov,
  number = {arXiv:2511.21631},
  eprint = {2511.21631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.21631},
  urldate = {2025-12-11},
  abstract = {We introduce Qwen3-VL, the most capable vision--language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency--quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and crossreferencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial--temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision--language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. To balance text-only and multimodal learning objectives, we apply square-root reweighting, which boosts multimodal performance without compromising text capabilities. We extend pretraining to a context length of 256K tokens and bifurcate post-training into non-thinking and thinking variants to address distinct application requirements. Furthermore, we allocate additional compute resources to the post-training phase to further enhance model performance. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-ofExperts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Bai et al. - 2025 - Qwen3-VL Technical Report.pdf}
}

@misc{baLayerNormalization2016,
  title = {Layer Normalization},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = 2016,
  eprint = {1607.06450},
  archiveprefix = {arXiv}
}

@book{banachewiczKaggleBookData2022,
  title = {The Kaggle Book: Data Analysis and Machine Learning for Competitive Data Science},
  author = {Banachewicz, Konrad and Massaron, Luca},
  year = 2022,
  edition = {[First edition]},
  publisher = {Packt Publishing},
  address = {Birmingham}
}

@article{baptistaRelationPrognosticsPredictor2022,
  title = {Relation between Prognostics Predictor Evaluation Metrics and Local Interpretability Shap Values},
  author = {Baptista, Marcia L. and Goebel, Kai and Henriques, Elsa M.P.},
  year = 2022,
  journal = {Artificial Intelligence},
  volume = {306},
  pages = {103667},
  doi = {10.1016/j.artint.2022.103667}
}

@article{Barndorff-Nielsen_2005,
  title = {Econometrics of Testing for Jumps in Financial Economics Using Bipower Variation},
  author = {{Barndorff-Nielsen}, Ole E. and Shephard, Neil},
  year = 2005,
  journal = {Journal of Financial Econometrics},
  doi = {10.1093/jjfinec/nbi022},
  mag_id = {2136215272},
  pmcid = {null},
  pmid = {null}
}

@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable Artificial Intelligence (Xai): Concepts, Taxonomies, Opportunities and Challenges toward Responsible Ai},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = 2020,
  journal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  doi = {10.1016/j.inffus.2019.12.012},
  urldate = {2023-04-08}
}

@article{basakPredictingDirectionStock2019,
  title = {Predicting the Direction of Stock Market Prices Using Tree-Based Classifiers},
  author = {Basak, Suryoday and Kar, Saibal and Saha, Snehanshu and Khaidem, Luckyson and Dey, Sudeepa Roy},
  year = 2019,
  journal = {The North American Journal of Economics and Finance},
  volume = {47},
  pages = {552--567},
  doi = {10.1016/j.najef.2018.06.013}
}

@inproceedings{bastingsElephantInterpretabilityRoom2020,
  title = {The Elephant in the Interpretability Room: Why Use Attention as Explanation When We Have Saliency Methods?},
  booktitle = {Proceedings of the {{Third BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Bastings, Jasmijn and Filippova, Katja},
  year = 2020,
  pages = {149--155},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.blackboxnlp-1.14},
  urldate = {2023-01-07}
}

@misc{batesCrossvalidationWhatDoes2022,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  year = 2022,
  eprint = {2104.00673},
  archiveprefix = {arXiv}
}

@article{Battalio_2006,
  title = {Options and the Bubble},
  author = {Battalio, Robert H. and Schultz, Paul H.},
  year = 2006,
  journal = {Journal of Finance},
  doi = {10.2139/ssrn.558543},
  mag_id = {1984118538},
  pmcid = {null},
  pmid = {null}
}

@misc{benalcazarSyntheticIDCard2022,
  title = {Synthetic {{ID Card Image Generation}} for {{Improving Presentation Attack Detection}}},
  author = {Benalcazar, Daniel and Tapia, Juan E. and Gonzalez, Sebastian and Busch, Christoph},
  year = 2022,
  month = oct,
  number = {arXiv:2211.00098},
  eprint = {2211.00098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.00098},
  urldate = {2024-11-28},
  abstract = {Currently, it is ever more common to access online services for activities which formerly required physical attendance. From banking operations to visa applications, a significant number of processes have been digitised, especially since the advent of the COVID-19 pandemic, requiring remote biometric authentication of the user. On the downside, some subjects intend to interfere with the normal operation of remote systems for personal profit by using fake identity documents, such as passports and ID cards. Deep learning solutions to detect such frauds have been presented in the literature. However, due to privacy concerns and the sensitive nature of personal identity documents, developing a dataset with the necessary number of examples for training deep neural networks is challenging. This work explores three methods for synthetically generating ID card images to increase the amount of data while training frauddetection networks. These methods include computer vision algorithms and Generative Adversarial Networks. Our results indicate that databases can be supplemented with synthetic images without any loss in performance for the print/scan Presentation Attack Instrument Species (PAIS) and a loss in performance of 1\% for the screen capture PAIS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Benalcazar et al. - 2022 - Synthetic ID Card Image Generation for Improving Presentation Attack Detection.pdf}
}

@article{bengioNeuralProbabilisticLanguage,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = 2003,
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {6},
  pages = {1137--1155}
}

@incollection{bengioPracticalRecommendationsGradientBased2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Bengio, Yoshua},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = 2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--478},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8\_26}
}

@inproceedings{bennettExploitingUnlabeledData2002,
  title = {Exploiting Unlabeled Data in Ensemble Methods},
  booktitle = {Proceedings of the Eighth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Bennett, Kristin P. and Demiriz, Ayhan and Maclin, Richard},
  year = 2002,
  pages = {289--296},
  publisher = {ACM},
  address = {Edmonton, Canada},
  doi = {10.1145/775047.775090},
  urldate = {2023-04-23}
}

@article{berkmanLargeOptionTrades1996,
  title = {Large Option Trades, Market Makers, and Limit Orders},
  author = {Berkman, Henk},
  year = 1996,
  journal = {The Review of Financial Studies},
  volume = {9},
  number = {3},
  pages = {977--1002},
  publisher = {[Oxford University Press, Society for Financial Studies]},
  doi = {10.1093/rfs/9.3.977},
  urldate = {2023-05-18},
  jstor = {2962317}
}

@article{bessembinderBidAskSpreadsMeasuring,
  title = {Bid-Ask Spreads: Measuring Trade Execution Costs in Financial Markets},
  author = {Bessembinder, Hendrik}
}

@article{bessembinderIssuesAssessingTrade2003,
  title = {Issues in Assessing Trade Execution Costs},
  author = {Bessembinder, Hendrik},
  year = 2003,
  journal = {Journal of Financial Markets},
  volume = {6},
  number = {3},
  pages = {233--257},
  doi = {10.1016/S1386-4181(02)00064-2}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = 2006,
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York}
}

@article{Black_1975,
  title = {Fact and Fantasy in the Use of Options},
  author = {Black, Fischer},
  year = 1975,
  journal = {Financial Analysts Journal},
  doi = {10.2469/faj.v31.n4.36},
  mag_id = {1816654404},
  pmcid = {null},
  pmid = {null}
}

@article{blackPricingOptionsCorporate1973,
  title = {The Pricing of Options and Corporate Liabilities},
  author = {Black, Fischer and Scholes, Myron},
  year = 1973,
  journal = {The Journal of Political Economy},
  volume = {81},
  number = {3},
  pages = {637--654},
  doi = {10.1086/260062},
  jstor = {1831029}
}

@article{blazejewskiLocalNonParametricModel2005,
  title = {A Local Non-Parametric Model for Trade Sign Inference},
  author = {Blazejewski, Adam and Coggins, Richard},
  year = 2005,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {348},
  pages = {481--495},
  doi = {10.1016/j.physa.2004.09.033}
}

@article{Bloomfield_1999,
  title = {Market Transparency: Who Wins and Who Loses?},
  author = {Bloomfield, Robert J. and O'Hara, Maureen},
  year = 1999,
  journal = {Review of Financial Studies},
  doi = {10.1093/rfs/12.1.5},
  mag_id = {2022730840},
  pmcid = {null},
  pmid = {null}
}

@misc{boardofgovernorsofthefederalreservesystemus1YearTreasuryBill1959,
  title = {1-Year Treasury Bill Secondary Market Rate},
  author = {{Board of Governors of the Federal Reserve System (US)}},
  year = 1959,
  urldate = {2021-10-31}
}

@article{Boehmer_2007,
  title = {Estimating the Probability of Informed Trading - Does Trade Misclassification Matter?},
  author = {Boehmer, Ekkehart and Grammig, Joachim and Theissen, Erik},
  year = 2007,
  journal = {Journal of Financial Markets},
  doi = {10.1016/j.finmar.2006.07.002},
  mag_id = {2132988645},
  pmcid = {null},
  pmid = {null}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching Word Vectors with Subword Information},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = 2017,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  doi = {10.1162/tacl\_a\_00051}
}

@article{bojerLearningsKaggleForecasting,
  title = {Learnings from Kaggle's Forecasting Competitions},
  author = {Bojer, Casper Solheim and Meldgaard, Jens Peder}
}

@misc{boleyBetterShortGreedy2021,
  title = {Better Short than Greedy: Interpretable Models through Optimal Rule Boosting},
  author = {Boley, Mario and Teshuva, Simon and Bodic, Pierre Le and Webb, Geoffrey I.},
  year = 2021,
  eprint = {2101.08380},
  urldate = {2023-02-27},
  archiveprefix = {arXiv}
}

@misc{bonedSyntheticDatasetID2024,
  title = {Synthetic Dataset of {{ID}} and {{Travel Document}}},
  author = {Boned, Carlos and Talarmain, Maxime and Ghanmi, Nabil and Chiron, Guillaume and Biswas, Sanket and Awal, Ahmad Montaser and Terrades, Oriol Ramos},
  year = 2024,
  month = jan,
  number = {arXiv:2401.01858},
  eprint = {2401.01858},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01858},
  urldate = {2024-12-10},
  abstract = {This paper presents a new synthetic dataset of ID and travel documents, called SIDTD. The SIDTD dataset is created to help training and evaluating forged ID documents detection systems. Such a dataset has become a necessity as ID documents contain personal information and a public dataset of real documents can not be released. Moreover, forged documents are scarce, compared to legit ones, and the way they are generated varies from one fraudster to another resulting in a class of high intra-variability. In this paper we trained state-of-the-art models on this dataset and we compare them to the performance achieved in larger, but private, datasets. The creation of this dataset will help to document image analysis community to progress in the task of ID document verification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Boned et al. - 2024 - Synthetic dataset of ID and Travel Document.pdf;/Users/markusbilz/Zotero/storage/EKHDBKJV/2401.html}
}

@misc{bordesIntroductionVisionLanguageModeling2024,
  title = {An {{Introduction}} to {{Vision-Language Modeling}}},
  author = {Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C. and Bardes, Adrien and Petryk, Suzanne and Ma{\~n}as, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and Ibrahim, Mark and Hall, Melissa and Xiong, Yunyang and Lebensold, Jonathan and Ross, Candace and Jayakumar, Srihari and Guo, Chuan and Bouchacourt, Diane and {Al-Tahan}, Haider and Padthe, Karthik and Sharma, Vasu and Xu, Hu and Tan, Xiaoqing Ellen and Richards, Megan and Lavoie, Samuel and Astolfi, Pietro and Hemmat, Reyhane Askari and Chen, Jun and Tirumala, Kushal and Assouel, Rim and Moayeri, Mazda and Talattof, Arjang and Chaudhuri, Kamalika and Liu, Zechun and Chen, Xilun and Garrido, Quentin and Ullrich, Karen and Agrawal, Aishwarya and Saenko, Kate and Celikyilmaz, Asli and Chandra, Vikas},
  year = 2024,
  month = may,
  number = {arXiv:2405.17247},
  eprint = {2405.17247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.17247},
  urldate = {2024-11-28},
  abstract = {Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Bordes et al. - 2024 - An Introduction to Vision-Language Modeling.pdf}
}

@article{borisovDeepNeuralNetworks2022,
  title = {Deep {{Neural Networks}} and {{Tabular Data}}: {{A Survey}}},
  author = {Borisov, Vadim and Leemann, Tobias and Sessler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = 2022,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--21},
  doi = {10.1109/TNNLS.2022.3229161}
}

@article{boweNewClassicalBayesian,
  title = {New Classical and Bayesian Estimators for Classifying Trade Direction in the Absence of Quotes},
  author = {Bowe, Michael and Cho, Sungjun and Hyde, Stuart and Sung, Iljin},
  year = 2018,
  pages = {78}
}

@article{boxAnalysisTransformations2022,
  title = {An Analysis of Transformations},
  author = {Box, G E P and Cox, D R},
  year = 1964,
  journal = {Journal of the Royal Statistical Society},
  series = {Series {{B}} ({{Methodological}})},
  volume = {26},
  number = {2},
  pages = {211--252}
}

@article{breedenPricesStateContingentClaims1978,
  title = {Prices of State-Contingent Claims Implicit in Option Prices},
  author = {Breeden, Douglas T. and Litzenberger, Robert H.},
  year = 1978,
  journal = {The Journal of Business},
  volume = {51},
  number = {4},
  pages = {621},
  doi = {10.1086/296025}
}

@article{breimanBaggingPredictors1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = 1996,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  doi = {10.1007/BF00058655},
  urldate = {2021-07-13}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = 1984,
  edition = {1},
  publisher = {CLC Press},
  address = {Boca Raton, FL, USA}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = 2001,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  doi = {10.1023/A:1010933404324}
}

@misc{breuelEffectsHyperparametersSGD2015,
  title = {The Effects of Hyperparameters on {{SGD}} Training of Neural Networks},
  author = {Breuel, Thomas M.},
  year = 2015,
  eprint = {1508.02788},
  urldate = {2022-10-25},
  archiveprefix = {arXiv}
}

@book{bringhurstElementsTypographicStyle2004,
  title = {The Elements of Typographic Style},
  author = {Bringhurst, Robert},
  year = 2004,
  edition = {3rd ed},
  publisher = {Hartley \& Marks, Publishers},
  address = {Point Roberts, WA}
}

@misc{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = 2020,
  eprint = {2005.14165},
  urldate = {2023-01-09},
  archiveprefix = {arXiv}
}

@article{burkeHybridRecommenderSystems2002,
  title = {Hybrid Recommender Systems: Survey and Experiments\dag},
  author = {Burke, Robin},
  year = 2002,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {12},
  number = {4},
  pages = {331--370},
  doi = {10.1023/A:1021240730564},
  urldate = {2021-05-06}
}

@article{busariCrudeOilPrice2021,
  title = {Crude Oil Price Prediction: A Comparison between {{AdaBoost-LSTM}} and {{AdaBoost-GRU}} for Improving Forecasting Performance},
  author = {Busari, Ganiyu Adewale and Lim, Dong Hoon},
  year = 2021,
  journal = {Computers \& Chemical Engineering},
  volume = {155},
  pages = {107513},
  doi = {10.1016/j.compchemeng.2021.107513},
  urldate = {2022-07-12}
}

@article{caoInformationalContentOption2005,
  title = {Informational Content of Option Volume Prior to Takeovers},
  author = {Cao, Charles and Chen, Zhiwu and Griffin, John M.},
  year = 2005,
  journal = {The Journal of Business},
  volume = {78},
  number = {3},
  pages = {1073--1109},
  doi = {10.1086/429654}
}

@misc{carionEndtoendObjectDetection2020,
  title = {End-to-End Object Detection with Transformers},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  year = 2020,
  eprint = {2005.12872},
  urldate = {2023-01-18},
  archiveprefix = {arXiv}
}

@misc{carliniQuantifyingMemorizationNeural2023,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = 2023,
  month = mar,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07646},
  urldate = {2025-12-04},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Carlini et al. - 2023 - Quantifying Memorization Across Neural Language Models.pdf}
}

@article{carrionTradeSigningFast2020,
  title = {Trade Signing in Fast Markets},
  author = {Carrion, Allen and Kolay, Madhuparna},
  year = 2020,
  journal = {Financial Review},
  volume = {55},
  number = {3},
  pages = {385--404},
  doi = {10.1111/fire.12218}
}

@misc{caruanaObtainingCalibratedProbabilities,
  title = {Obtaining Calibrated Probabilities from Boosting},
  author = {Caruana, Alexandru Niculescu-Mizil Rich}
}

@article{carvalhoMachineLearningInterpretability2019,
  title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
  author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  year = 2019,
  journal = {Electronics},
  volume = {8},
  number = {8},
  pages = {832},
  doi = {10.3390/electronics8080832},
  urldate = {2023-04-08}
}

@article{cerdaEncodingHighcardinalityString2022,
  title = {Encoding High-Cardinality String Categorical Variables},
  author = {Cerda, Patricio and Varoquaux, Ga{\"e}l},
  year = 2022,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {3},
  eprint = {1907.01860},
  pages = {1164--1176},
  doi = {10.1109/TKDE.2020.2992529},
  urldate = {2023-01-28},
  archiveprefix = {arXiv}
}

@article{chakrabartyEvaluatingTradeClassification2015,
  title = {Evaluating Trade Classification Algorithms: Bulk Volume Classification versus the Tick Rule and the Lee-Ready Algorithm},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = 2015,
  journal = {Journal of Financial Markets},
  volume = {25},
  pages = {52--79},
  doi = {10.1016/j.finmar.2015.06.001}
}

@article{chakrabartyTradeClassificationAlgorithms2007,
  title = {Trade Classification Algorithms for Electronic Communications Network Trades},
  author = {Chakrabarty, Bidisha and Li, Bingguang and Nguyen, Vanthuan and Van Ness, Robert A.},
  year = 2007,
  journal = {Journal of Banking \& Finance},
  volume = {31},
  number = {12},
  pages = {3806--3821},
  doi = {10.1016/j.jbankfin.2007.03.003}
}

@article{chakrabartyTradeClassificationAlgorithms2012,
  title = {Trade Classification Algorithms: A Horse Race between the Bulk-Based and the Tick-Based Rules},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = 2012,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2182819}
}

@article{chakravartyInformedTradingStock2004,
  title = {Informed {{Trading}} in {{Stock}} and {{Option Markets}}},
  author = {Chakravarty, Sugato and Gulen, Huseyin and Mayhew, Stewart},
  year = 2004,
  journal = {The Journal of Finance},
  volume = {59},
  number = {3},
  pages = {1235--1257},
  doi = {10.1111/j.1540-6261.2004.00661.x},
  urldate = {2023-06-02}
}

@article{Chan_1995,
  title = {The Behavior of Stock Prices around Institutional Trades},
  author = {Chan, Louis K.C. and Lakonishok, Josef},
  year = 1995,
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1995.tb04053.x},
  mag_id = {1992426919},
  pmcid = {null},
  pmid = {null}
}

@article{Chan_2002,
  title = {The Informational Role of Stock and Option Volume},
  author = {Chan, Kalok and Chung, Y. Peter and Fong, Wai-Ming},
  year = 2002,
  journal = {Review of Financial Studies},
  doi = {10.2139/ssrn.170356},
  mag_id = {2158240268},
  pmcid = {null},
  pmid = {null}
}

@article{chanAlgorithmicTrading,
  title = {Algorithmic Trading},
  author = {Chan, Ernie},
  pages = {225},
  doi = {10.1002/9781118676998}
}

@misc{chanTransformersGeneralizeDifferently2022,
  title = {Transformers Generalize Differently from Information Stored in Context vs in Weights},
  author = {Chan, Stephanie C. Y. and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K. and Hill, Felix},
  year = 2022,
  eprint = {2210.05675},
  urldate = {2022-10-18},
  archiveprefix = {arXiv}
}

@inproceedings{chapelleSemiSupervisedClassificationLow2005,
  title = {Semi-Supervised Classification by Low Density Separation},
  booktitle = {Proceedings of the {{Tenth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chapelle, Olivier and Zien, Alexander},
  year = 2005,
  pages = {57--64}
}

@book{chapelleSemisupervisedLearning2006,
  title = {Semi-Supervised Learning},
  author = {Chapelle, Olivier and Sch{\"o}lkopf, Bernhard and Zien, Alexander},
  year = 2006,
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA}
}

@article{chaumUntraceableElectronicMail1981,
  title = {Untraceable Electronic Mail, Return Addresses, and Digital Pseudonyms},
  author = {Chaum, David L.},
  year = 1981,
  journal = {Communications of the ACM},
  volume = {24},
  number = {2},
  pages = {84--90},
  doi = {10.1145/358549.358563},
  urldate = {2022-10-14}
}

@inproceedings{cheferGenericAttentionmodelExplainability2021,
  title = {Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = 2021,
  pages = {387--396},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00045}
}

@inproceedings{cheferTransformerInterpretabilityAttention2021,
  title = {Transformer Interpretability beyond Attention Visualization},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = 2021,
  pages = {782--791},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00084},
  urldate = {2023-01-08}
}

@misc{chenAlgorithmsEstimateShapley2022,
  title = {Algorithms to Estimate {{Shapley}} Value Feature Attributions},
  author = {Chen, Hugh and Covert, Ian C. and Lundberg, Scott M. and Lee, Su-In},
  year = 2022,
  eprint = {2207.07605},
  urldate = {2023-05-23},
  archiveprefix = {arXiv}
}

@inproceedings{chenDebiasedSelfTrainingSemiSupervised2022,
  title = {Debiased Self-Training for Semi-Supervised Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Baixu and Jiang, Junguang and Wang, Ximei and Wan, Pengfei and Wang, Jianmin and Long, Mingsheng},
  year = 2022,
  volume = {36},
  pages = {32424--32437},
  publisher = {Curran Associates, Inc.},
  address = {New Orleans, LA, USA}
}

@misc{chenDeepLearningAsset2021,
  title = {Deep Learning in Asset Pricing},
  author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
  year = 2021,
  eprint = {1904.00745},
  archiveprefix = {arXiv}
}

@article{chenDemandCrashInsurance2019,
  title = {Demand for {{Crash Insurance}}, {{Intermediary Constraints}}, and {{Risk Premia}} in {{Financial Markets}}},
  author = {Chen, Hui and Joslin, Scott and Ni, Sophie Xiaoyan},
  year = 2019,
  journal = {The Review of Financial Studies},
  volume = {32},
  number = {1},
  pages = {228--265},
  doi = {10.1093/rfs/hhy004},
  urldate = {2023-06-26}
}

@misc{chenExcelFormerNeuralNetwork2023,
  title = {{{ExcelFormer}}: A Neural Network Surpassing Gbdts on Tabular Data},
  author = {Chen, Jintai and Yan, Jiahuan and Chen, Danny Ziyi and Wu, Jian},
  year = 2023,
  eprint = {2301.02819},
  urldate = {2023-01-14},
  archiveprefix = {arXiv}
}

@misc{chenExpandingPerformanceBoundaries2025,
  title = {Expanding {{Performance Boundaries}} of {{Open-Source Multimodal Models}} with {{Model}}, {{Data}}, and {{Test-Time Scaling}}},
  author = {Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and Gu, Lixin and Wang, Xuehui and Li, Qingyun and Ren, Yimin and Chen, Zixuan and Luo, Jiapeng and Wang, Jiahao and Jiang, Tan and Wang, Bo and He, Conghui and Shi, Botian and Zhang, Xingcheng and Lv, Han and Wang, Yi and Shao, Wenqi and Chu, Pei and Tu, Zhongying and He, Tong and Wu, Zhiyong and Deng, Huipeng and Ge, Jiaye and Chen, Kai and Zhang, Kaipeng and Wang, Limin and Dou, Min and Lu, Lewei and Zhu, Xizhou and Lu, Tong and Lin, Dahua and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  year = 2025,
  month = jan,
  number = {arXiv:2412.05271},
  eprint = {2412.05271},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05271},
  urldate = {2025-04-04},
  abstract = {We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70\% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Chen et al. - 2025 - Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Sc.pdf}
}

@inproceedings{chengWideDeepLearning2016,
  title = {Wide \& Deep Learning for Recommender Systems},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Deep Learning}} for {{Recommender Systems}}},
  author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
  year = 2016,
  pages = {7--10},
  publisher = {ACM},
  address = {Boston MA USA},
  doi = {10.1145/2988450.2988454},
  urldate = {2023-01-26}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = 2020,
  volume = {119},
  pages = {1597--1607},
  publisher = {PMLR},
  address = {Virtual}
}

@misc{chenTrainingDeepNets2016,
  title = {Training Deep Nets with Sublinear Memory Cost},
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  year = 2016,
  eprint = {1604.06174},
  urldate = {2022-11-23},
  archiveprefix = {arXiv}
}

@misc{chenTrueModelTrue2020,
  title = {True to the Model or True to the Data?},
  author = {Chen, Hugh and Janizek, Joseph D. and Lundberg, Scott and Lee, Su-In},
  year = 2020,
  eprint = {2006.16234},
  urldate = {2023-04-09},
  archiveprefix = {arXiv}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: A Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  editor = {{Krishnapuram,Balaji} and {Shah, Mohak} and {Smola, Alexander J.} and {Aggarwal, Charu C.} and {Shen, Dou} and {Rastogi, Rajeev}},
  year = 2016,
  pages = {785--794},
  address = {San Francisco, CA, USA},
  doi = {10.1145/2939672.2939785}
}

@article{choiEstimationBidAskSpreads1988,
  title = {On the {{Estimation}} of {{Bid-Ask Spreads}}: {{Theory}} and {{Evidence}}},
  author = {Choi, J. Y. and Salandro, Dan and Shastri, Kuldeep},
  year = 1988,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {23},
  number = {2},
  pages = {219--230},
  publisher = {Cambridge University Press},
  doi = {10.2307/2330882}
}

@misc{cholakovGatedTabTransformerEnhancedDeep2022,
  title = {The {{GatedTabTransformer}}. {{An}} Enhanced Deep Learning Architecture for Tabular Modeling},
  author = {Cholakov, Radostin and Kolev, Todor},
  year = 2022,
  eprint = {2201.00199},
  urldate = {2023-01-11},
  archiveprefix = {arXiv}
}

@article{chordiaIndexOptionTrading2021,
  title = {Index {{Option Trading Activity}} and {{Market Returns}}},
  author = {Chordia, Tarun and Kurov, Alexander and Muravyev, Dmitriy and Subrahmanyam, Avanidhar},
  year = 2021,
  journal = {Management Science},
  volume = {67},
  number = {3},
  pages = {1758--1778},
  doi = {10.1287/mnsc.2019.3529},
  urldate = {2023-06-20}
}

@misc{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = 2017,
  eprint = {1706.03741},
  urldate = {2021-09-23},
  archiveprefix = {arXiv}
}

@article{chuanSuccessAdaBoostIts2021,
  title = {The Success of {{AdaBoost}} and Its Application in Portfolio Management},
  author = {Chuan, Yijian and Zhao, Chaoyi and He, Zhenrui and Wu, Lan},
  year = 2021,
  journal = {International Journal of Financial Engineering},
  volume = {08},
  number = {02},
  pages = {2142001},
  doi = {10.1142/S2424786321420019},
  urldate = {2022-07-12}
}

@inproceedings{clarkElectraPretrainingText2020,
  title = {{{ELECTRA}}: Pre-Training Text Encoders as Discriminators Rather than Generators},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = 2020,
  address = {Online}
}

@inproceedings{clarkWhatDoesBERT2019,
  title = {What Does {{BERT}} Look at? {{An}} Analysis of {{BERT}}'s Attention},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  editor = {{Linzen, Tal} and {Chrupala, Grzegorz} and {Belinkov, Yonatan} and {Hupkes, Dieuwke}},
  year = 2019,
  pages = {276--286},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4828}
}

@misc{coenenVisualizingMeasuringGeometry2019,
  title = {Visualizing and {{Measuring}} the {{Geometry}} of {{BERT}}},
  author = {Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = 2019,
  eprint = {1906.02715},
  archiveprefix = {arXiv}
}

@article{collin-dufresneInformedTradingStock2021,
  title = {Informed {{Trading}} in the {{Stock Market}} and {{Option-Price Discovery}}},
  author = {{Collin-Dufresne}, Pierre and Fos, Vyacheslav and Muravyev, Dmitry},
  year = 2021,
  journal = {Journal of Financial and Quantitative Analysis},
  volume = {56},
  number = {6},
  pages = {1945--1984},
  doi = {10.1017/S0022109020000629},
  urldate = {2023-06-26}
}

@misc{congDEEPSEQUENCEMODELING,
  title = {Deep Sequence Modeling: Development and Applications in Asset Pricing},
  author = {Cong, Lin William and Tang, Ke and Wang, Jingyuan and Zhang, Yang}
}

@article{Cont_2013,
  title = {Price Dynamics in a Markovian Limit Order Market},
  author = {Cont, Rama and {de Larrard}, Adrien},
  year = 2013,
  journal = {Siam Journal on Financial Mathematics},
  doi = {10.1137/110856605},
  mag_id = {3121639422},
  pmcid = {null},
  pmid = {null}
}

@misc{CosineSimilarityEmbeddingsReally,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  urldate = {2025-11-11},
  howpublished = {https://arxiv.org/html/2403.05440v1},
  file = {/Users/markusbilz/Zotero/storage/9W5PXAUI/2403.html}
}

@article{covertExplainingRemovingUnified,
  title = {Explaining by {{Removing}}: {{A Unified Framework}} for {{Model Explanation}}},
  author = {Covert, Ian C}
}

@inproceedings{covertUnderstandingGlobalFeature2020,
  title = {Understanding Global Feature Contributions with Additive Importance Measures},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Covert, Ian and Lundberg, Scott M and Lee, Su-In},
  year = 2020,
  volume = {33},
  pages = {17212--17223},
  publisher = {Curran Associates, Inc.},
  address = {Online}
}

@article{cowgillAlgorithmicFairnessEconomics,
  title = {Algorithmic Fairness and Economics},
  author = {Cowgill, Bo and Tucker, Catherine},
  pages = {31}
}

@incollection{coxExploratoryDataAnalysis2017,
  title = {Exploratory Data Analysis: What Data Do {{I}} Have?},
  booktitle = {Translating {{Statistics}} to {{Make Decisions}}},
  author = {Cox, Victoria},
  year = 2017,
  pages = {47--74},
  publisher = {Apress},
  address = {Berkeley, CA},
  doi = {10.1007/978-1-4842-2256-0\_3},
  urldate = {2023-01-22},
  collaborator = {Cox, Victoria}
}

@article{coxOptionPricingSimplified1979,
  title = {Option Pricing: A Simplified Approach},
  author = {Cox, John C and Ross, A},
  year = 1979,
  pages = {35},
  doi = {10.1016/0304-405X(79)90015-1}
}

@article{coxRelationForwardPrices1981,
  title = {The Relation between Forward Prices and Futures Prices},
  author = {Cox, John C.},
  year = 1981,
  journal = {Journal of Financial Economics},
  pages = {321--346},
  doi = {10.1016/0304-405X(81)90002-7}
}

@article{creamerAutomatedTradingBoosting2010,
  title = {Automated Trading with Boosting and Expert Weighting},
  author = {Creamer, Germ{\'a}n and Freund, Yoav},
  year = 2010,
  journal = {Quantitative Finance},
  volume = {10},
  number = {4},
  pages = {401--420},
  doi = {10.1080/14697680903104113},
  urldate = {2022-07-12}
}

@article{crspDATADESCRIPTIONSGUIDE,
  title = {Data Descriptions Guide Crsp Us Stock \& Us Index Databases},
  author = {{CRSP}},
  pages = {130}
}

@misc{culurcielloFallRNNLSTM2019,
  title = {The Fall of {{RNN}} / {{LSTM}}},
  author = {Culurciello, Eugenio},
  year = 2019,
  urldate = {2021-12-03}
}

@misc{culurcielloMemoryAttentionSequences2018,
  title = {Memory, Attention, Sequences},
  author = {Culurciello, Eugenio},
  year = 2018,
  urldate = {2021-12-03}
}

@misc{CurriculumLearning,
  title = {Curriculum Learning},
  doi = {10.1145/1553374.1553380},
  urldate = {2025-12-27},
  howpublished = {https://dl.acm.org/doi/epdf/10.1145/1553374.1553380},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Curriculum learning.pdf;/Users/markusbilz/Zotero/storage/ERAIDN58/1553374.html}
}

@article{daiEmbeddingLearning2022,
  title = {Embedding Learning},
  author = {Dai, Ben and Shen, Xiaotong and Wang, Junhui},
  year = 2022,
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {537},
  pages = {307--319},
  doi = {10.1080/01621459.2020.1775614},
  urldate = {2023-02-24}
}

@misc{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: Attentive Language Models beyond a Fixed-Length Context},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = 2019,
  eprint = {1901.02860},
  urldate = {2023-01-11},
  archiveprefix = {arXiv}
}

@inproceedings{dalche-bucSemisupervisedMarginBoost2001,
  title = {Semi-Supervised {{MarginBoost}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{d' Alch{\'e}-Buc}, Florence and Grandvalet, Yves and Ambroise, Christophe},
  year = 2001,
  volume = {14},
  pages = {553--560},
  publisher = {MIT Press},
  address = {Vancouver, Canada}
}

@misc{darabiContrastiveMixupSelf2021,
  title = {Contrastive Mixup: Self- and Semi-Supervised Learning for Tabular Domain},
  author = {Darabi, Sajad and Fazeli, Shayan and Pazoki, Ali and Sankararaman, Sriram and Sarrafzadeh, Majid},
  year = 2021,
  eprint = {2108.12296},
  urldate = {2023-01-24},
  archiveprefix = {arXiv}
}

@misc{dauphinLanguageModelingGated2017,
  title = {Language Modeling with Gated Convolutional Networks},
  author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
  year = 2017,
  eprint = {1612.08083},
  urldate = {2023-01-17},
  archiveprefix = {arXiv}
}

@article{davisGradientBoostingQuantitative2019,
  title = {Gradient Boosting for Quantitative Finance},
  author = {Davis, Jesse and Devos, Laurens and Reyners, Sofie and Schoutens, Wim},
  year = 2019,
  journal = {The Journal of Computational Finance},
  volume = {24},
  number = {4},
  pages = {4},
  doi = {10.21314/JCF.2020.403}
}

@misc{dehghaniUniversalTransformers2019,
  title = {Universal Transformers},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  year = 2019,
  eprint = {1807.03819},
  archiveprefix = {arXiv}
}

@article{deisenrothMathematicsMachineLearning,
  title = {Mathematics for Machine Learning},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  pages = {417},
  doi = {10.1017/9781108679930}
}

@misc{deitkeMolmoPixMoOpen2024,
  title = {Molmo and {{PixMo}}: {{Open Weights}} and {{Open Data}} for {{State-of-the-Art Vision-Language Models}}},
  shorttitle = {Molmo and {{PixMo}}},
  author = {Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and Lu, Jiasen and Anderson, Taira and Bransom, Erin and Ehsani, Kiana and Ngo, Huong and Chen, YenSung and Patel, Ajay and Yatskar, Mark and {Callison-Burch}, Chris and Head, Andrew and Hendrix, Rose and Bastani, Favyen and VanderBilt, Eli and Lambert, Nathan and Chou, Yvonne and Chheda, Arnavi and Sparks, Jenna and Skjonsberg, Sam and Schmitz, Michael and Sarnat, Aaron and Bischoff, Byron and Walsh, Pete and Newell, Chris and Wolters, Piper and Gupta, Tanmay and Zeng, Kuo-Hao and Borchardt, Jon and Groeneveld, Dirk and Nam, Crystal and Lebrecht, Sophie and Wittlif, Caitlin and Schoenick, Carissa and Michel, Oscar and Krishna, Ranjay and Weihs, Luca and Smith, Noah A. and Hajishirzi, Hannaneh and Girshick, Ross and Farhadi, Ali and Kembhavi, Aniruddha},
  year = 2024,
  month = dec,
  number = {arXiv:2409.17146},
  eprint = {2409.17146},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.17146},
  urldate = {2024-12-09},
  abstract = {Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a welltuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{dengStrategicTradingManipulation,
  title = {Strategic Trading and Manipulation: Machine Learning in Limit Order Markets},
  author = {Deng, Xinyi and He, Xue-Zhong}
}

@misc{dettmersQLoRAEfficientFinetuning2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = 2023,
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2024-11-26},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters\textasciitilde (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = 2019,
  volume = {1},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, MN},
  doi = {10.18653/v1/N19-1423}
}

@article{dieboldComparingPredictiveAccuracy1995,
  title = {Comparing Predictive Accuracy},
  author = {Diebold, Francis X. and Mariano, Roberto S.},
  year = 1995,
  journal = {Journal of Business \& Economic Statistics},
  volume = {13},
  number = {3},
  pages = {253--263},
  doi = {10.1080/07350015.1995.10524599}
}

@misc{dongInternLMXComposer24KHDPioneeringLarge2024,
  title = {{{InternLM-XComposer2-4KHD}}: {{A Pioneering Large Vision-Language Model Handling Resolutions}} from 336 {{Pixels}} to {{4K HD}}},
  shorttitle = {{{InternLM-XComposer2-4KHD}}},
  author = {Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and Yan, Hang and Gao, Yang and Chen, Zhe and Zhang, Xinyue and Li, Wei and Li, Jingwen and Wang, Wenhai and Chen, Kai and He, Conghui and Zhang, Xingcheng and Dai, Jifeng and Qiao, Yu and Lin, Dahua and Wang, Jiaqi},
  year = 2024,
  month = apr,
  number = {arXiv:2404.06512},
  eprint = {2404.06512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.06512},
  urldate = {2025-12-11},
  abstract = {The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 \texttimes{} 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLMXComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 \texttimes{} 1600) and beyond. Concurrently, considering the ultrahigh resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 \texttimes{} 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT4V and Gemini Pro in 10 of the 16 benchmarks. The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at https://github. com/InternLM/InternLM-XComposer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Dong et al. - 2024 - InternLM-XComposer2-4KHD A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pix.pdf}
}

@inproceedings{dongTablePretrainingSurvey2022,
  title = {Table Pre-Training: A Survey on Model Architectures, Pre-Training Objectives, and Downstream Tasks},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Dong, Haoyu and Cheng, Zhoujun and He, Xinyi and Zhou, Mengyu and Zhou, Anda and Zhou, Fan and Liu, Ao and Han, Shi and Zhang, Dongmei},
  year = 2022,
  pages = {5426--5435},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Vienna, Austria},
  doi = {10.24963/ijcai.2022/761},
  urldate = {2023-01-25}
}

@article{dorogushCatBoostGradientBoosting,
  title = {{{CatBoost}}: Gradient Boosting with Categorical Features Support},
  author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey}
}

@inproceedings{dosovitskiyImageWorth16x162021,
  title = {An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = 2021,
  address = {Online}
}

@misc{duTextCrafterAccuratelyRendering2025,
  title = {{{TextCrafter}}: {{Accurately Rendering Multiple Texts}} in {{Complex Visual Scenes}}},
  shorttitle = {{{TextCrafter}}},
  author = {Du, Nikai and Chen, Zhennan and Chen, Zhizhou and Gao, Shan and Chen, Xi and Jiang, Zhengkai and Yang, Jian and Tai, Ying},
  year = 2025,
  month = apr,
  number = {arXiv:2503.23461},
  eprint = {2503.23461},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23461},
  urldate = {2025-04-07},
  abstract = {This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Du et al. - 2025 - TextCrafter Accurately Rendering Multiple Texts in Complex Visual Scenes.pdf}
}

@misc{duTextCrafterAccuratelyRendering2025a,
  title = {{{TextCrafter}}: {{Accurately Rendering Multiple Texts}} in {{Complex Visual Scenes}}},
  shorttitle = {{{TextCrafter}}},
  author = {Du, Nikai and Chen, Zhennan and Gao, Shan and Chen, Zhizhou and Chen, Xi and Jiang, Zhengkai and Yang, Jian and Tai, Ying},
  year = 2025,
  month = aug,
  number = {arXiv:2503.23461},
  eprint = {2503.23461},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23461},
  urldate = {2025-11-12},
  abstract = {This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Du et al. - 2025 - TextCrafter Accurately Rendering Multiple Texts in Complex Visual Scenes 1.pdf}
}

@article{Easley_1996,
  title = {Liquidity, Information, and Infrequently Traded Stocks},
  author = {Easley, David and Kiefer, Nicholas M. and O'Hara, Maureen and Paperman, Joseph B.},
  year = 1996,
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1996.tb04074.x},
  mag_id = {2100746131},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_2002,
  title = {Is Information Risk a Determinant of Asset Returns},
  author = {Easley, David and Hvidkjaer, Soeren and O'Hara, Maureen},
  year = 2002,
  journal = {Journal of Finance},
  doi = {10.1111/1540-6261.00493},
  mag_id = {2100821524},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_2012,
  title = {Bulk Classification of Trading Activity},
  author = {Easley, David and {de Prado}, Marcos Lopez and O'Hara, Maureen},
  year = 2012,
  journal = {null},
  doi = {null},
  mag_id = {1547818237},
  pmcid = {null},
  pmid = {null}
}

@article{easleyDiscerningInformationTrade2016,
  title = {Discerning Information from Trade Data},
  author = {Easley, David and {de Prado}, Marcos Lopez and O'Hara, Maureen},
  year = 2016,
  journal = {Journal of Financial Economics},
  volume = {120},
  number = {2},
  pages = {269--285},
  doi = {10.1016/j.jfineco.2016.01.018}
}

@article{easleyFlowToxicityLiquidity2012,
  title = {Flow Toxicity and Liquidity in a High-Frequency World},
  author = {Easley, David and {L{\'o}pez de Prado}, Marcos M. and O'Hara, Maureen},
  year = 2012,
  journal = {Review of Financial Studies},
  volume = {25},
  number = {5},
  pages = {1457--1493},
  doi = {10.1093/rfs/hhs053},
  urldate = {2022-10-09}
}

@article{easleyMicrostructureFlashCrash2011,
  title = {The Microstructure of the ``Flash Crash'': {\emph{Flow Toxicity, Liquidity Crashes, and the Probability of Informed Trading}}},
  author = {Easley, David and {L{\'o}pez de Prado}, Marcos M. and O'Hara, Maureen},
  year = 2011,
  journal = {The Journal of Portfolio Management},
  volume = {37},
  number = {2},
  pages = {118--128},
  doi = {10.3905/jpm.2011.37.2.118},
  urldate = {2023-02-10}
}

@article{easleyOptionVolumeStock1998,
  title = {Option Volume and Stock Prices: Evidence on Where Informed Traders Trade},
  author = {Easley, David and O'Hara, Maureen and Srinivas, P.S.},
  year = 1998,
  journal = {The Journal of Finance},
  volume = {53},
  number = {2},
  pages = {431--465},
  doi = {10.1111/0022-1082.194060}
}

@article{elhage2021mathematical,
  title = {A Mathematical Framework for Transformer Circuits},
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = 2021,
  journal = {Transformer Circuits Thread},
  urldate = {2023-01-11}
}

@article{ellisAccuracyTradeClassification2000,
  title = {The Accuracy of Trade Classification Rules: Evidence from Nasdaq},
  author = {Ellis, Katrina and Michaely, Roni and O'Hara, Maureen},
  year = 2000,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  pages = {529--551},
  doi = {10.2307/2676254}
}

@article{enguehardSemiSupervisedLearningDeep2019,
  title = {Semi-Supervised Learning with Deep Embedded Clustering for Image Classification and Segmentation},
  author = {Enguehard, Joseph and O'Halloran, Peter and Gholipour, Ali},
  year = 2019,
  journal = {IEEE access : practical innovations, open solutions},
  volume = {7},
  pages = {11093--11104},
  doi = {10.1109/ACCESS.2019.2891970},
  urldate = {2023-04-14}
}

@article{erhanWhyDoesUnsupervised,
  title = {Why Does Unsupervised Pre-Training Help Deep Learning?},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  year = 2010,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {19},
  pages = {625--660}
}

@misc{ExplainingIndividualPredictions,
  title = {Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values \textbar{} Elsevier Enhanced Reader},
  doi = {10.1016/j.artint.2021.103502},
  urldate = {2023-03-26}
}

@book{falkPracticalRecommenderSystems2019,
  title = {Practical Recommender Systems},
  author = {Falk, Kim},
  year = 2019,
  publisher = {Manning},
  address = {Shelter Island, NY}
}

@article{famaCAPMWantedDead1996,
  title = {The {{CAPM}} Is Wanted, Dead or Alive},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = 1996,
  journal = {The Journal of Finance},
  volume = {51},
  number = {5},
  pages = {1947--1958},
  doi = {10.1111/j.1540-6261.1996.tb05233.x},
  urldate = {2021-10-29}
}

@article{famaCommonRiskFactors1993,
  title = {Common Risk Factors in the Returns on Stocks and Bonds},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = 1993,
  journal = {Journal of Financial Economics},
  volume = {33},
  number = {1},
  pages = {3--56},
  doi = {10.1016/0304-405X(93)90023-5},
  urldate = {2021-10-29}
}

@article{famaFivefactorAssetPricing2015,
  title = {A Five-Factor Asset Pricing Model},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = 2015,
  journal = {Journal of Financial Economics},
  volume = {116},
  number = {1},
  pages = {1--22},
  doi = {10.1016/j.jfineco.2014.10.010},
  urldate = {2021-10-31}
}

@misc{fanScalingLanguageFreeVisual2025,
  title = {Scaling {{Language-Free Visual Representation Learning}}},
  author = {Fan, David and Tong, Shengbang and Zhu, Jiachen and Sinha, Koustuv and Liu, Zhuang and Chen, Xinlei and Rabbat, Michael and Ballas, Nicolas and LeCun, Yann and Bar, Amir and Xie, Saining},
  year = 2025,
  month = apr,
  number = {arXiv:2504.01017},
  eprint = {2504.01017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.01017},
  urldate = {2025-04-03},
  abstract = {Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Fan et al. - 2025 - Scaling Language-Free Visual Representation Learning.pdf}
}

@article{fardRecommenderSystemBased2013,
  title = {Recommender System Based on Semantic Similarity},
  author = {Fard, Karamollah Bagheri and Nilashi, Mehrbakhsh and Rahmani, Mohsen and Ibrahim, Othman},
  year = 2013,
  volume = {3},
  number = {6},
  pages = {11}
}

@article{fawziDiscoveringFasterMatrix2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and {Romera-Paredes}, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  year = 2022,
  journal = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  doi = {10.1038/s41586-022-05172-4},
  urldate = {2022-10-10}
}

@article{fedeniaMachineLearningCorporate2021,
  title = {Machine Learning in the Corporate Bond Market and beyond: A New Classifier},
  author = {Fedenia, Mark A. and Nam, Seunghan and Ronen, Tavy},
  year = 2021,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3848068}
}

@misc{federalreservebankofstlouisNBERBasedRecession2022,
  title = {{{NBER}} Based Recession Indicators for the United States from the Period Following the Peak through the Trough [{{USREC}}]},
  author = {{Federal Reserve Bank of St. Louis}},
  year = 2022,
  urldate = {2022-07-26}
}

@article{feldhutterSameBondDifferent2012,
  title = {The Same Bond at Different Prices: Identifying Search Frictions and Selling Pressures},
  author = {Feldh{\"u}tter, Peter},
  year = 2012,
  journal = {The Review of Financial Studies},
  volume = {25},
  number = {4},
  pages = {1155--1206},
  doi = {10.1093/rfs/hhr093},
  urldate = {2022-12-30}
}

@misc{fengDeepLearningPredicting2018,
  title = {Deep Learning for Predicting Asset Returns},
  author = {Feng, Guanhao and He, Jingyu and Polson, Nicholas G.},
  year = 2018,
  eprint = {1804.09314},
  archiveprefix = {arXiv}
}

@article{fengLogtransformationItsImplications2014,
  title = {Log-Transformation and Its Implications for Data Analysis},
  author = {Feng, Changyong and Wang, Hongyue and Lu, Naiji and Chen, Tian and He, Hua and Lu, Ying and Tu, Xin M},
  year = 2014,
  volume = {26},
  number = {2},
  pages = {6}
}

@misc{fiedlerSimpleModificationsImprove2021,
  title = {Simple Modifications to Improve Tabular Neural Networks},
  author = {Fiedler, James},
  year = 2021,
  eprint = {2108.03214},
  archiveprefix = {arXiv}
}

@article{finucaneDirectTestMethods2000,
  title = {A Direct Test of Methods for Inferring Trade Direction from Intra-Day Data},
  author = {Finucane, Thomas J.},
  year = 2000,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  pages = {553--576},
  doi = {10.2307/2676255}
}

@article{fisherAllModelsAre,
  title = {All Models Are Wrong, but Many Are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca}
}

@article{fletcherSophisticatedInvestorsFederal1988,
  title = {Sophisticated Investors under the Federal Securities Laws},
  author = {Fletcher, C. Edward},
  year = 1988,
  journal = {Duke Law Journal},
  volume = {1988},
  number = {6},
  pages = {1081},
  doi = {10.2307/1372532},
  urldate = {2023-02-04},
  jstor = {1372532}
}

@article{frazziniBettingBeta2014,
  title = {Betting against Beta},
  author = {Frazzini, Andrea and Pedersen, Lasse Heje},
  year = 2014,
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {1},
  pages = {1--25},
  doi = {10.1016/j.jfineco.2013.10.005},
  urldate = {2020-11-15}
}

@article{freeboroughInvestigatingExplainabilityMethods2022,
  title = {Investigating {{Explainability Methods}} in {{Recurrent Neural Network Architectures}} for {{Financial Time Series Data}}},
  author = {Freeborough, Warren and Van Zyl, Terence},
  year = 2022,
  journal = {Applied Sciences},
  volume = {12},
  number = {3},
  pages = {1427},
  doi = {10.3390/app12031427},
  urldate = {2023-05-22}
}

@article{freybergerDissectingCharacteristicsNonparametrically,
  title = {Dissecting Characteristics Nonparametrically},
  author = {Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael},
  year = 2020,
  journal = {The Review of Financial Studies},
  volume = {5},
  number = {33},
  pages = {2326--2377},
  doi = {10.1093/rfs/hhz123}
}

@article{friedmanAdditiveLogisticRegression2000,
  title = {Additive Logistic Regression: A Statistical View of Boosting (with Discussion and a Rejoinder by the Authors)},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = 2000,
  journal = {The Annals of Statistics},
  volume = {28},
  number = {2},
  doi = {10.1214/aos/1016218223}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: A Gradient Boosting Machine.},
  author = {Friedman, Jerome},
  year = 2001,
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  pages = {1189--1232},
  doi = {10.1214/aos/1013203451}
}

@article{friedmanStochasticGradientBoosting2002,
  title = {Stochastic Gradient Boosting},
  author = {Friedman, Jerome},
  year = 2002,
  journal = {Computational Statistics \& Data Analysis},
  volume = {38},
  number = {4},
  pages = {367--378},
  doi = {10.1016/S0167-9473(01)00065-2}
}

@article{frommelAccuracyTradeClassification2021,
  title = {The Accuracy of Trade Classification Systems on the Foreign Exchange Market: Evidence from the {{RUB}}/{{USD}} Market},
  author = {Fr{\"o}mmel, Michael and D'Hoore, Dick and Lampaert, Kevin},
  year = 2021,
  journal = {Finance Research Letters},
  volume = {42},
  pages = {101892},
  doi = {10.1016/j.frl.2020.101892},
  urldate = {2023-02-01}
}

@misc{gabrielDynamicPricingUsing2021,
  title = {Dynamic Pricing Using Reinforcement Learning and Neural Networks},
  author = {Gabriel, Reslley},
  year = 2021,
  urldate = {2022-01-14}
}

@article{garleanuDemandBasedOptionPricing2009,
  title = {Demand-Based Option Pricing},
  author = {G{\^a}rleanu, Nicolae and Pedersen, Lasse Heje and Poteshman, Allen M.},
  year = 2009,
  journal = {Review of Financial Studies},
  volume = {22},
  number = {10},
  pages = {4259--4299},
  doi = {10.1093/rfs/hhp005}
}

@misc{gengUnmetPromiseSynthetic2024,
  title = {The {{Unmet Promise}} of {{Synthetic Training Images}}: {{Using Retrieved Real Images Performs Better}}},
  shorttitle = {The {{Unmet Promise}} of {{Synthetic Training Images}}},
  author = {Geng, Scott and Hsieh, Cheng-Yu and Ramanujan, Vivek and Wallingford, Matthew and Li, Chun-Liang and Koh, Pang Wei and Krishna, Ranjay},
  year = 2024,
  month = jul,
  number = {arXiv:2406.05184},
  eprint = {2406.05184},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05184},
  urldate = {2024-12-11},
  abstract = {Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. What additional value does the intermediate generator provide over directly training on relevant parts of the upstream data? Grounding this question in the setting of image classification, we compare finetuning on task-relevant, targeted synthetic data generated by Stable Diffusion---a generative model trained on the LAION-2B dataset---against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that retrieval is a critical baseline to consider when training with synthetic data---a baseline that current methods do not yet surpass. We release code, data, and models at https://github.com/scottgeng00/unmet-promise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Geng et al. - 2024 - The Unmet Promise of Synthetic Training Images Using Retrieved Real Images Performs Better.pdf}
}

@misc{gengUnmetPromiseSynthetic2025,
  title = {The {{Unmet Promise}} of {{Synthetic Training Images}}: {{Using Retrieved Real Images Performs Better}}},
  shorttitle = {The {{Unmet Promise}} of {{Synthetic Training Images}}},
  author = {Geng, Scott and Hsieh, Cheng-Yu and Ramanujan, Vivek and Wallingford, Matthew and Li, Chun-Liang and Koh, Pang Wei and Krishna, Ranjay},
  year = 2025,
  month = jan,
  number = {arXiv:2406.05184},
  eprint = {2406.05184},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05184},
  urldate = {2025-11-12},
  abstract = {Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data? Grounding this question in the setting of image classification, we compare finetuning on task-relevant, targeted synthetic data generated by Stable Diffusion---a generative model trained on the LAION-2B dataset---against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that targeted retrieval is a critical baseline to consider when training with synthetic data---a baseline that current methods do not yet surpass. We release code, data, and models at https://github.com/scottgeng00/unmet-promise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Geng et al. - 2025 - The Unmet Promise of Synthetic Training Images Using Retrieved Real Images Performs Better.pdf}
}

@misc{gevaTransformerFeedforwardLayers2021,
  title = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  year = 2021,
  eprint = {2012.14913},
  urldate = {2023-01-16},
  archiveprefix = {arXiv}
}

@article{GivingSoftwareIts2019,
  title = {Giving Software Its Due},
  year = 2019,
  journal = {Nature Methods},
  volume = {16},
  number = {3},
  pages = {207--207},
  doi = {10.1038/s41592-019-0350-x},
  urldate = {2022-12-08}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = 2011,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {15},
  pages = {315--323},
  publisher = {PMLR},
  address = {Fort Lauderdale, FL, USA}
}

@article{Glosten_1988,
  title = {Estimating the Components of the Bid/Ask Spread},
  author = {Glosten, Lawrence R. and Harris, Lawrence},
  year = 1988,
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(88)90034-7},
  mag_id = {2035606631},
  pmcid = {null},
  pmid = {null}
}

@article{goettlerEquilibriumDynamicLimit2005,
  title = {Equilibrium in a Dynamic Limit Order Market},
  author = {Goettler, Ronald L. and Parlour, Christine A. and Rajan, Uday},
  year = 2005,
  journal = {The Journal of Finance},
  volume = {60},
  number = {5},
  pages = {2149--2192},
  doi = {10.1111/j.1540-6261.2005.00795.x}
}

@article{goncalves-pintoWhyOptionPrices2020,
  title = {Why {{Do Option Prices Predict Stock Returns}}? {{The Role}} of {{Price Pressure}} in the {{Stock Market}}},
  author = {{Goncalves-Pinto}, Luis and Grundy, Bruce D. and Hameed, Allaudeen and Van Der Heijden, Thijs and Zhu, Yichao},
  year = 2020,
  journal = {Management Science},
  volume = {66},
  number = {9},
  pages = {3903--3926},
  doi = {10.1287/mnsc.2019.3398},
  urldate = {2023-06-23}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = 2016,
  publisher = {MIT Press}
}

@book{goossensLaTeXGraphicsCompanion2008,
  title = {The Latex Graphics Companion},
  editor = {Goossens, Michel},
  year = 2008,
  series = {Addison-{{Wesley}} Series on Tools and Techniques for Computer Typesetting},
  edition = {2nd ed},
  publisher = {Addison-Wesley},
  address = {Upper Saddle River, NJ}
}

@inproceedings{gorishniyEmbeddingsNumericalFeatures2022,
  title = {On Embeddings for Numerical Features in Tabular Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = 2022,
  volume = {35},
  pages = {24991--25004},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{gorishniyRevisitingDeepLearning2021,
  title = {Revisiting Deep Learning Models for Tabular Data},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = 2021,
  volume = {34},
  pages = {18932--18943},
  publisher = {Curran Associates, Inc.},
  address = {Red Hook, NY}
}

@misc{GradientBoostPart,
  title = {Gradient Boost Part 1 (of 4): Regression Main Ideas - {{YouTube}}},
  urldate = {2021-12-25}
}

@article{grammigDivergingRoadsTheoryBased2020,
  title = {Diverging Roads: Theory-Based vs. Machine Learning-Implied Stock Risk Premia},
  author = {Grammig, Joachim and Hanenberg, Constantin and Schlag, Christian and S{\"o}nksen, Jantje},
  year = 2020,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3536835},
  urldate = {2021-11-03}
}

@misc{grauerOptionTradeClassification2022,
  title = {Option Trade Classification},
  author = {Grauer, Caroline and Schuster, Philipp and {Uhrig-Homburg}, Marliese},
  year = 2023,
  eprint = {ssrn.4098475},
  doi = {10.2139/ssrn.4098475},
  archiveprefix = {SSRN}
}

@misc{griffinZeroShotCoresetSelection2024,
  title = {Zero-{{Shot Coreset Selection}}: {{Efficient Pruning}} for {{Unlabeled Data}}},
  shorttitle = {Zero-{{Shot Coreset Selection}}},
  author = {Griffin, Brent A. and Marks, Jacob and Corso, Jason J.},
  year = 2024,
  month = nov,
  number = {arXiv:2411.15349},
  eprint = {2411.15349},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15349},
  urldate = {2024-12-10},
  abstract = {Deep learning increasingly relies on massive data with substantial costs for storage, annotation, and model training. To reduce these costs, coreset selection aims to find a representative subset of data to train models while ideally performing on par with the full data training. State-ofthe-art coreset methods use carefully-designed criteria to quantify the importance of each data example via ground truth labels and dataset-specific training, then select examples whose scores lie in a certain range to construct a coreset. These methods work well in their respective settings, however, they cannot select data that are unlabeled, which is the majority of real-world data. To that end, this paper motivates and formalizes the problem of unlabeled coreset selection to enable greater scale and reduce annotation costs for deep learning. As a solution, we develop Zero-Shot Coreset Selection (ZCore), a method that efficiently selects coresets without ground truth labels or training on candidate data. Instead, ZCore uses existing foundation models to generate a zero-shot embedding space for unlabeled data, then quantifies the relative importance of each example based on overall coverage and redundancy within the embedding distribution. We evaluate ZCore on four datasets and outperform several state-of-theart label-based methods, leading to a strong baseline for future research in unlabeled coreset selection. On ImageNet, ZCore selections achieve a downstream model accuracy of 53.99\% with only 10\% training data, which outperforms label-based methods while removing annotation requirements for 1.15 million images. Our code is publicly available at https://github.com/voxel51/zcore.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Griffin et al. - 2024 - Zero-Shot Coreset Selection Efficient Pruning for Unlabeled Data.pdf}
}

@book{grimmKinderUndHausmaerchen2015,
  title = {{Kinder- und Hausm\"archen - Edition Zulu-Ebooks.com}},
  author = {Grimm, Jacob und Wilhelm},
  year = 2015,
  month = jun,
  publisher = {Edition Zulu-Ebooks.com},
  abstract = {{$<$}p{$>$}Layout neu bearbeitet f\"ur Zulu Ebooks (Mai 2017 A.B.){$<$}/p{$>$}},
  langid = {ngerman},
  keywords = {Abenteuerroman,Fantasyroman,Gesellschaft,Kinder und Jugend,Kurzgeschichte,Marchen und Sagen},
  annotation = {Item ID: \_:n0},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Grimm - 2015 - Kinder- und Hausmärchen - Edition Zulu-Ebooks.com.epub}
}

@inproceedings{grinsztajnWhyTreebasedModels2022,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular Data?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = 2022,
  series = {{{NeurIPS}} 2022},
  volume = {36},
  pages = {507--520},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY}
}

@misc{guanIDNetNovelDataset2024,
  title = {{{IDNet}}: {{A Novel Dataset}} for {{Identity Document Analysis}} and {{Fraud Detection}}},
  shorttitle = {{{IDNet}}},
  author = {Guan, Hong and Wang, Yancheng and Xie, Lulu and Nag, Soham and Goel, Rajeev and Swamy, Niranjan Erappa Narayana and Yang, Yingzhen and Xiao, Chaowei and Prisby, Jonathan and Maciejewski, Ross and Zou, Jia},
  year = 2024,
  month = sep,
  number = {arXiv:2408.01690},
  eprint = {2408.01690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.01690},
  urldate = {2024-12-10},
  abstract = {Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy. In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection efforts. The IDNet dataset comprises 837,060 images of synthetically generated identity documents, totaling approximately 490 gigabytes, categorized into 20 types from \$10\$ U.S. states and 10 European countries. We evaluate the utility and present use cases of the dataset, illustrating how it can aid in training privacy-preserving fraud detection methods, facilitating the generation of camera and video capturing of identity documents, and testing schema unification and other identity document management functionalities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Guan et al. - 2024 - IDNet A Novel Dataset for Identity Document Analysis and Fraud Detection 1.pdf;/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Guan et al. - 2024 - IDNet A Novel Dataset for Identity Document Analysis and Fraud Detection.pdf;/Users/markusbilz/Zotero/storage/3PMRDT9E/2408.html}
}

@article{guAutoencoderAssetPricing2021,
  title = {Autoencoder Asset Pricing Models},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = 2021,
  journal = {Journal of Econometrics},
  volume = {222},
  number = {1},
  pages = {429--450},
  doi = {10.1016/j.jeconom.2020.07.009}
}

@article{guEmpiricalAssetPricing2020,
  title = {Empirical Asset Pricing via Machine Learning},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = 2020,
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {5},
  pages = {2223--2273},
  doi = {10.1093/rfs/hhaa009},
  urldate = {2021-10-19}
}

@article{gunnarssonDeepLearningCredit2021,
  title = {Deep Learning for Credit Scoring: Do or Don't?},
  author = {Gunnarsson, Bj{\"o}rn Rafn and {vanden Broucke}, Seppe and Baesens, Bart and {\'O}skarsd{\'o}ttir, Mar{\'i}a and Lemahieu, Wilfried},
  year = 2021,
  journal = {European Journal of Operational Research},
  volume = {295},
  number = {1},
  pages = {292--305},
  doi = {10.1016/j.ejor.2021.03.006},
  urldate = {2022-01-12}
}

@inproceedings{guoEmbeddingLearningFramework2021,
  title = {An Embedding Learning Framework for Numerical Features in {{CTR}} Prediction},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Guo, Huifeng and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Li, Zhenguo and He, Xiuqiang},
  year = 2021,
  eprint = {2012.08986},
  pages = {2910--2918},
  doi = {10.1145/3447548.3467077},
  urldate = {2023-01-14},
  archiveprefix = {arXiv}
}

@misc{guoEntityEmbeddingsCategorical2016,
  title = {Entity Embeddings of Categorical Variables},
  author = {Guo, Cheng and Berkhahn, Felix},
  year = 2016,
  eprint = {1604.06737},
  urldate = {2023-01-25},
  archiveprefix = {arXiv}
}

@misc{guptaXTNestedTokenization2024,
  title = {{{xT}}: {{Nested Tokenization}} for {{Larger Context}} in {{Large Images}}},
  shorttitle = {{{xT}}},
  author = {Gupta, Ritwik and Li, Shufan and Zhu, Tyler and Malik, Jitendra and Darrell, Trevor and Mangalam, Karttikeya},
  year = 2024,
  month = jul,
  number = {arXiv:2403.01915},
  eprint = {2403.01915},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.01915},
  urldate = {2025-11-15},
  abstract = {Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. xT is a streaming, two-stage architecture that adapts existing vision backbones and long sequence language models to effectively model large images without quadratic memory growth. We are able to increase accuracy by up to 8.6\% on challenging classification tasks and F1 score by 11.6 on context-dependent segmentation on images as large as 29,000 x 29,000 pixels.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Gupta et al. - 2024 - xT Nested Tokenization for Larger Context in Large Images.pdf}
}

@inproceedings{gyamerahStockMarketMovement2019,
  title = {On Stock Market Movement Prediction via Stacking Ensemble Learning Method},
  booktitle = {2019 {{IEEE Conference}} on {{Computational Intelligence}} for {{Financial Engineering}} \& {{Economics}} ({{CIFEr}})},
  author = {Gyamerah, Samuel Asante and Ngare, Philip and Ikpe, Dennis},
  year = 2019,
  pages = {1--8},
  publisher = {IEEE},
  address = {Shenzhen, China},
  doi = {10.1109/CIFEr.2019.8759062},
  urldate = {2022-07-12}
}

@article{hagstromerBiasEffectiveBidask2021,
  title = {Bias in the Effective Bid-Ask Spread},
  author = {Hagstr{\"o}mer, Bj{\"o}rn},
  year = 2021,
  journal = {Journal of Financial Economics},
  volume = {142},
  number = {1},
  pages = {314--337},
  doi = {10.1016/j.jfineco.2021.04.018},
  urldate = {2023-02-25}
}

@inproceedings{hanAutoEncoderInspiredUnsupervised2018,
  title = {{{AutoEncoder}} Inspired Unsupervised Feature Selection},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Han, Kai and Wang, Yunhe and Zhang, Chao and Li, Chao and Xu, Chao},
  year = 2018,
  eprint = {1710.08310},
  address = {Calgary, AB},
  doi = {10.1109/ICASSP.2018.8462261},
  archiveprefix = {arXiv}
}

@article{hancockSurveyCategoricalData2020,
  title = {Survey on Categorical Data for Neural Networks},
  author = {Hancock, John T. and Khoshgoftaar, Taghi M.},
  year = 2020,
  journal = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {28},
  doi = {10.1186/s40537-020-00305-w}
}

@misc{hansBeGoldfishDont2024,
  title = {Be like a {{Goldfish}}, {{Don}}'t {{Memorize}}! {{Mitigating Memorization}} in {{Generative LLMs}}},
  author = {Hans, Abhimanyu and Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Kazemi, Hamid and Singhania, Prajwal and Singh, Siddharth and Somepalli, Gowthami and Geiping, Jonas and Bhatele, Abhinav and Goldstein, Tom},
  year = 2024,
  month = nov,
  number = {arXiv:2406.10209},
  eprint = {2406.10209},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10209},
  urldate = {2025-12-01},
  abstract = {Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Hans et al. - 2024 - Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs.pdf}
}

@article{harrisDayEndTransactionPrice1989,
  title = {A Day-End Transaction Price Anomaly},
  author = {Harris, Lawrence},
  year = 1989,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {24},
  number = {1},
  pages = {29},
  doi = {10.2307/2330746}
}

@article{harveyMultivariateStochasticVariance1994,
  title = {Multivariate Stochastic Variance Models},
  author = {Harvey, A. and Ruiz, E. and Shephard, N.},
  year = 1994,
  journal = {The Review of Economic Studies},
  volume = {61},
  number = {2},
  pages = {247--264},
  doi = {10.2307/2297980},
  urldate = {2020-12-12}
}

@article{harveyTestingEqualityPrediction1997,
  title = {Testing the Equality of Prediction Mean Squared Errors},
  author = {Harvey, David and Leybourne, Stephen and Newbold, Paul},
  year = 1997,
  journal = {International Journal of Forecasting},
  volume = {13},
  number = {2},
  pages = {281--291},
  doi = {10.1016/S0169-2070(96)00719-4}
}

@article{hasbrouckTradesQuotesInventories1988,
  title = {Trades, Quotes, Inventories, and Information},
  author = {Hasbrouck, Joel},
  year = 1988,
  journal = {Journal of Financial Economics},
  volume = {22},
  number = {2},
  pages = {229--252},
  doi = {10.1016/0304-405X(88)90070-0}
}

@article{hasbrouckTradingCostsReturns2009,
  title = {Trading Costs and Returns for {{U}}.s. {{Equities}}: Estimating Effective Costs from Daily Data},
  author = {Hasbrouck, Joel},
  year = 2009,
  journal = {The Journal of Finance},
  volume = {64},
  number = {3},
  pages = {1445--1477},
  doi = {10.1111/j.1540-6261.2009.01469.x},
  urldate = {2023-02-26}
}

@book{hastietrevorElementsStatisticalLearning2009,
  title = {The Elements of Statistical Learning},
  author = {Hastie, Trevor, Sami and Friedman, Harry and Tibshirani, Robert},
  year = 2009,
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  address = {New York, NY}
}

@inproceedings{hazimehTreeEnsembleLayer2020,
  title = {The {{Tree Ensemble Layer}}: {{Differentiability}} Meets {{Conditional Computation}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Hazimeh, Hussein and Ponomareva, Natalia and Mol, Petros and Tan, Zhenyu and Mazumder, Rahul},
  year = 2020,
  pages = {4138--4148},
  publisher = {PMLR},
  urldate = {2023-05-15}
}

@inproceedings{heatonEmpiricalAnalysisFeature2016,
  title = {An Empirical Analysis of Feature Engineering for Predictive Modeling},
  booktitle = {{{SoutheastCon}} 2016},
  author = {Heaton, Jeff},
  year = 2016,
  pages = {1--6},
  doi = {10.1109/SECON.2016.7506650}
}

@misc{heBagTricksImage2018,
  title = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = 2018,
  eprint = {1812.01187},
  urldate = {2022-10-26},
  archiveprefix = {arXiv}
}

@misc{heDeepResidualLearning2015,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2015,
  eprint = {1512.03385},
  archiveprefix = {arXiv}
}

@article{heegaardPhDResearchPlan,
  title = {{{PhD}} Research Plan},
  author = {Heegaard, Poul}
}

@misc{hegselmannTabLLMFewshotClassification2022,
  title = {{{TabLLM}}: Few-Shot Classification of Tabular Data with Large Language Models},
  author = {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  year = 2022,
  eprint = {2210.10723},
  urldate = {2023-01-14},
  archiveprefix = {arXiv}
}

@misc{hendrycksGaussianErrorLinear2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = 2020,
  eprint = {1606.08415},
  archiveprefix = {arXiv}
}

@article{hennessyComputerArchitecture,
  title = {Computer {{Architecture}}},
  author = {Hennessy, John L},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Hennessy - Computer Architecture 1.pdf}
}

@misc{hertzPrompttopromptImageEditing2022,
  title = {Prompt-to-Prompt Image Editing with Cross Attention Control},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and {Cohen-Or}, Daniel},
  year = 2022,
  eprint = {2208.01626},
  urldate = {2022-09-12},
  archiveprefix = {arXiv}
}

@article{hidasiRecurrentNeuralNetworks2018,
  title = {Recurrent Neural Networks with Top-k Gains for Session-Based Recommendations},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
  year = 2018,
  journal = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  eprint = {1706.03847},
  pages = {843--852},
  doi = {10.1145/3269206.3271761},
  urldate = {2021-05-04},
  archiveprefix = {arXiv}
}

@article{hintonForwardForwardAlgorithmPreliminary,
  title = {The Forward-Forward Algorithm: Some Preliminary Investigations},
  author = {Hinton, Geoffrey}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = 2012,
  eprint = {1207.0580},
  archiveprefix = {arXiv}
}

@article{hirtEndtoendProcessModel,
  title = {An End-to-End Process Model for Supervised Machine Learning Classification: From Problem to Deployment in Information Systems},
  author = {Hirt, Robin and Kuhl, Niklas and Satzger, Gerhard},
  pages = {9}
}

@article{hoangMachineLearningMethods,
  title = {Machine Learning Methods in Finance: Recent Applications and Prospects},
  author = {Hoang, Daniel and Wiegratz, Kevin},
  pages = {64}
}

@misc{hoAxialAttentionMultidimensional2019,
  title = {Axial Attention in Multidimensional Transformers},
  author = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  year = 2019,
  eprint = {1912.12180},
  urldate = {2022-01-04},
  archiveprefix = {arXiv}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = 1997,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2021-12-25}
}

@inproceedings{hoffmannTrainingComputeOptimalLarge2022,
  title = {An Empirical Analysis of Compute-Optimal Large Language Model Training},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and {de Las Casas}, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and {van den Driessche}, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Kar{\'e}n and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = 2022,
  volume = {35},
  pages = {30016--30030},
  publisher = {Curran Associates, Inc.}
}

@article{holdenLiquidityMeasurementProblems2014,
  title = {Liquidity Measurement Problems in Fast, Competitive Markets: Expensive and Cheap Solutions: Liquidity Measurement Problems in Fast, Competitive Markets},
  author = {Holden, Craig W. and Jacobsen, Stacey},
  year = 2014,
  journal = {The Journal of Finance},
  volume = {69},
  number = {4},
  pages = {1747--1785},
  doi = {10.1111/jofi.12127},
  urldate = {2023-02-06}
}

@article{holthausenEffectLargeBlock1987,
  title = {The Effect of Large Block Transactions on Security Prices: A Cross-Sectional Analysis},
  author = {Holthausen, Robert W. and Leftwich, Richard W. and Mayers, David},
  year = 1987,
  journal = {Journal of Financial Economics},
  volume = {19},
  number = {2},
  pages = {237--267},
  doi = {10.1016/0304-405X(87)90004-3}
}

@book{holzingerXxAIExplainableAI2022,
  title = {{{xxAI}} - {{Beyond Explainable AI}}: {{International Workshop}}, {{Held}} in {{Conjunction}} with {{ICML}} 2020, {{July}} 18, 2020, {{Vienna}}, {{Austria}}, {{Revised}} and {{Extended Papers}}},
  editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = 2022,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13200},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-04083-2},
  urldate = {2023-05-22}
}

@article{hookerUnrestrictedPermutationForces2021,
  title = {Unrestricted Permutation Forces Extrapolation: Variable Importance Requires at Least One More Model, or There Is No Free Variable Importance},
  author = {Hooker, Giles and Mentch, Lucas and Zhou, Siyu},
  year = 2021,
  journal = {Statistics and Computing},
  volume = {31},
  number = {6},
  pages = {82},
  doi = {10.1007/s11222-021-10057-z},
  urldate = {2023-04-11}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = 1989,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2021-12-08}
}

@book{houselPsychologyMoney2020,
  title = {The {{Psychology}} of {{Money}}},
  author = {Housel, Morgan},
  year = 2020,
  month = jun,
  langid = {british},
  annotation = {Item ID: \_:n0},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Housel - 2020 - The Psychology of Money.epub}
}

@misc{huangSnapshotEnsemblesTrain2017,
  title = {Snapshot Ensembles: Train 1, Get {{M}} for Free},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  year = 2017,
  eprint = {1704.00109},
  archiveprefix = {arXiv}
}

@misc{huangTabTransformerTabularData2020,
  title = {{{TabTransformer}}: Tabular Data Modeling Using Contextual Embeddings},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  year = 2020,
  eprint = {2012.06678},
  archiveprefix = {arXiv}
}

@article{huDoesOptionTrading2014,
  title = {Does Option Trading Convey Stock Price Information?},
  author = {Hu, Jianfeng},
  year = 2014,
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {3},
  pages = {625--645},
  doi = {10.1016/j.jfineco.2013.12.004}
}

@book{hullOptionsFuturesOther2012,
  title = {Options, Futures, and Other Derivatives},
  author = {Hull, John},
  year = 2012,
  edition = {8th ed},
  publisher = {Prentice Hall},
  address = {Boston}
}

@book{huyenDesigningMachineLearning,
  title = {Designing Machine Learning Systems},
  author = {Huyen, Chip}
}

@book{hyndmanForecastingPrinciplesPractice2021,
  title = {Forecasting: Principles and Practice},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  year = 2021,
  edition = {3}
}

@article{inceINDIVIDUALEQUITYRETURN2006,
  title = {Individual Equity Return Data from Thomson Datastream: Handle with Care!},
  author = {Ince, Ozgur S. and Porter, R. Burt},
  year = 2006,
  journal = {Journal of Financial Research},
  volume = {29},
  number = {4},
  pages = {463--479},
  doi = {10.1111/j.1475-6803.2006.00189.x},
  urldate = {2021-10-29}
}

@misc{IndexOptionTrading,
  title = {Index {{Option Trading Activity}} and {{Market Returns}}},
  doi = {10.1287/mnsc.2019.3529},
  urldate = {2023-06-20}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = 2015,
  eprint = {1502.03167},
  archiveprefix = {arXiv}
}

@article{ismailfawazDeepLearningTime2019,
  title = {Deep Learning for Time Series Classification: A Review},
  author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = 2019,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  pages = {917--963},
  doi = {10.1007/s10618-019-00619-1},
  urldate = {2021-11-16}
}

@misc{ivanovDataMovementAll2021,
  title = {Data Movement Is All You Need: A Case Study on Optimizing Transformers},
  author = {Ivanov, Andrei and Dryden, Nikoli and {Ben-Nun}, Tal and Li, Shigang and Hoefler, Torsten},
  year = 2021,
  eprint = {2007.00072},
  urldate = {2023-01-15},
  archiveprefix = {arXiv}
}

@misc{izmailovAveragingWeightsLeads2019,
  title = {Averaging Weights Leads to Wider Optima and Better Generalization},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = 2019,
  eprint = {1803.05407},
  urldate = {2022-10-13},
  archiveprefix = {arXiv}
}

@inproceedings{jacobGroupLassoOverlap2009,
  title = {Group Lasso with Overlap and Graph Lasso},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}} - {{ICML}} '09},
  author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
  year = 2009,
  pages = {1--8},
  publisher = {ACM Press},
  address = {Montreal, Quebec, Canada},
  doi = {10.1145/1553374.1553431},
  urldate = {2021-10-01}
}

@misc{jainAttentionNotExplanation2019,
  title = {Attention Is Not Explanation},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = 2019,
  eprint = {1902.10186},
  archiveprefix = {arXiv}
}

@article{japkowiczClassImbalanceProblem2002,
  title = {The Class Imbalance Problem: A Systematic Study},
  author = {Japkowicz, Nathalie and Stephen, Shaju},
  year = 2002,
  journal = {Intelligent Data Analysis},
  volume = {6},
  number = {5},
  pages = {429--449},
  doi = {10.3233/IDA-2002-6504}
}

@book{jarmulPracticalDataPrivacy2023,
  title = {Practical Data Privacy: Enhancing Privacy and Security in Data},
  shorttitle = {Practical Data Privacy},
  author = {Jarmul, Katharine},
  year = 2023,
  edition = {First edition},
  publisher = {O'Reilly Media},
  address = {Sebastopol, CA},
  abstract = {Between major privacy regulations like the GDPR and CCPA and expensive and notorious data breaches, there has never been so much pressure to ensure data privacy. Unfortunately, integrating privacy into data systems is still complicated. This essential guide will give you a fundamental understanding of modern privacy building blocks, like differential privacy, federated learning, and encrypted computation. Based on hard-won lessons, this book provides solid advice and best practices for integrating breakthrough privacy-enhancing technologies into production systems},
  isbn = {978-1-0981-2946-0},
  lccn = {QA76.9.A25 J375 2023},
  keywords = {Computer software,Data protection,Logiciels,Privacy Right of,Protection de l'information (Informatique),Securite Mesures,Security measures},
  annotation = {OCLC: on1356034197},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Jarmul - 2023 - Practical data privacy enhancing privacy and security in data.epub}
}

@inproceedings{jawaharWhatDoesBERT2019,
  title = {What Does {{BERT}} Learn about the Structure of Language?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  year = 2019,
  pages = {3651--3657},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1356}
}

@inproceedings{jinDatadrivenApproachPredict2015,
  title = {A Data-Driven Approach to Predict Default Risk of Loan for Online Peer-to-Peer ({{P2P}}) Lending},
  booktitle = {2015 {{Fifth International Conference}} on {{Communication Systems}} and {{Network Technologies}}},
  author = {Jin, Yu and Zhu, Yudan},
  year = 2015,
  pages = {609--613},
  doi = {10.1109/CSNT.2015.25}
}

@misc{jinPruningEffectGeneralization2022,
  title = {Pruning's Effect on Generalization through the Lens of Training and Regularization},
  author = {Jin, Tian and Carbin, Michael and Roy, Daniel M. and Frankle, Jonathan and Dziugaite, Gintare Karolina},
  year = 2022,
  eprint = {2210.13738},
  urldate = {2023-01-15},
  archiveprefix = {arXiv}
}

@article{johnsonSurveyDeepLearning2019,
  title = {Survey on Deep Learning with Class Imbalance},
  author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
  year = 2019,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {27},
  doi = {10.1186/s40537-019-0192-5},
  urldate = {2022-11-09}
}

@book{jorgensonAlmanackNavalRavikant2020,
  title = {The {{Almanack}} of {{Naval Ravikant}}: {{A Guide}} to {{Wealth}} and {{Happiness}}},
  shorttitle = {The {{Almanack}} of {{Naval Ravikant}}},
  author = {Jorgenson, Eric},
  year = 2020,
  edition = {1st ed},
  publisher = {BookBaby},
  address = {Cork},
  isbn = {978-1-5445-1420-8},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Jorgenson - 2020 - The Almanack of Naval Ravikant A Guide to Wealth and Happiness.epub}
}

@misc{josseConsistencySupervisedLearning2020,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = 2020,
  eprint = {1902.06931},
  archiveprefix = {arXiv}
}

@article{jurkatisInferringTradeDirections2022,
  title = {Inferring Trade Directions in Fast Markets},
  author = {Jurkatis, Simon},
  year = 2022,
  journal = {Journal of Financial Markets},
  volume = {58},
  pages = {100635},
  doi = {10.1016/j.finmar.2021.100635}
}

@article{kadanBoundExpectedStock2020,
  title = {A Bound on Expected Stock Returns},
  author = {Kadan, Ohad and Tang, Xiaoxiao},
  editor = {Van Nieuwerburgh, Stijn},
  year = 2020,
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {4},
  pages = {1565--1617},
  doi = {10.1093/rfs/hhz075}
}

@inproceedings{kadraWelltunedSimpleNets2021,
  title = {Well-Tuned Simple Nets Excel on Tabular Datasets},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  year = 2021,
  volume = {34},
  pages = {23928--23941},
  publisher = {Curran Associates, Inc.}
}

@article{kaeckPriceImpactBid2022,
  title = {Price Impact versus Bid--Ask Spreads in the Index Option Market},
  author = {Kaeck, Andreas and Van Kervel, Vincent and Seeger, Norman J.},
  year = 2022,
  journal = {Journal of Financial Markets},
  volume = {59},
  pages = {100675},
  doi = {10.1016/j.finmar.2021.100675},
  urldate = {2023-06-20}
}

@misc{KatanamlorgInvoicesdonutmodelv1Main2023,
  title = {Katanaml-Org/Invoices-Donut-Model-v1 at Main},
  year = 2023,
  month = may,
  urldate = {2024-12-10},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/katanaml-org/invoices-donut-model-v1/tree/main},
  file = {/Users/markusbilz/Zotero/storage/82RHH6BU/main.html}
}

@article{kaufmanLeakageDataMining2012,
  title = {Leakage in Data Mining: Formulation, Detection, and Avoidance},
  author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia and Stitelman, Ori},
  year = 2012,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {6},
  number = {4},
  pages = {1--21},
  doi = {10.1145/2382577.2382579},
  urldate = {2023-02-24}
}

@article{kavajeczPackagingLiquidityBlind2005,
  title = {Packaging Liquidity: Blind Auctions and Transaction Efficiencies},
  author = {Kavajecz, Kenneth A. and Keim, Donald B.},
  year = 2005,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {40},
  number = {3},
  pages = {465--492},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0022109000001836},
  urldate = {2023-04-09},
  jstor = {27647208}
}

@inproceedings{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: A Highly Efficient Gradient Boosting Decision Tree},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = 2017,
  volume = {30},
  pages = {3146--3154},
  publisher = {Curran Associates, Inc.}
}

@article{kellyCharacteristicsAreCovariances2019,
  title = {Characteristics Are Covariances: A Unified Model of Risk and Return},
  author = {Kelly, Bryan T. and Pruitt, Seth and Su, Yinan},
  year = 2019,
  journal = {Journal of Financial Economics},
  volume = {134},
  number = {3},
  pages = {501--524},
  doi = {10.1016/j.jfineco.2019.05.001}
}

@inproceedings{keskarLargeBatchTrainingDeep2017,
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = 2017,
  address = {Toulon, France}
}

@article{khorramEndtoendCNNLSTM2021,
  title = {End-to-End {{CNN}} + {{LSTM}} Deep Learning Approach for Bearing Fault Diagnosis},
  author = {Khorram, Amin and Khalooei, Mohammad and Rezghi, Mansoor},
  year = 2021,
  journal = {Applied Intelligence},
  volume = {51},
  number = {2},
  pages = {736--751},
  doi = {10.1007/s10489-020-01859-1},
  urldate = {2022-05-03}
}

@article{kichererSeamlesslyPortableApplications2012,
  title = {Seamlessly Portable Applications: Managing the Diversity of Modern Heterogeneous Systems},
  author = {Kicherer, Mario and Nowak, Fabian and Buchty, Rainer and Karl, Wolfgang},
  year = 2012,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {8},
  number = {4},
  pages = {1--20},
  doi = {10.1145/2086696.2086721},
  urldate = {2021-02-20}
}

@inproceedings{kitaevReformerEfficientTransformer2020,
  title = {Reformer: The Efficient Transformer},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}}},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = 2020,
  address = {Addis Ababa, Ethopia}
}

@misc{klingenbrunnTransformerImplementationTimeseries2021,
  title = {Transformer Implementation for Time-Series Forecasting},
  author = {Klingenbrunn, Natasha},
  year = 2021,
  urldate = {2021-11-06}
}

@book{kommerImmobilienkaufUndFinanzierung2022,
  title = {{Immobilienkauf und Finanzierung f\"ur Selbstnutzer: Geld sparen und Fehler vermeiden beim Kauf der eigenen vier W\"ande}},
  shorttitle = {{Immobilienkauf und Finanzierung f\"ur Selbstnutzer}},
  author = {Kommer, Gerd},
  year = 2022,
  edition = {2., vollst\"andig \"uberarbeitete und erweiterete Auflage},
  publisher = {Campus Verlag},
  address = {Frankfurt New York},
  abstract = {Immobilien gelten als solide Wertanlage und seit Corona hat sich die Nachfrage nach eigenen vier W\"anden weiter verst\"arkt. Die Zinsen sind niedrig, die Preise regional sehr unterschiedlich. Und in den Kreditvertr\"agen der Banken wird das Kleingedruckte immer komplexer. Um die T\"ucken des Hauskaufs zu durchschauen, braucht es Sachverstand. Dieses Buch hilft, diesen zu entwickeln: bei der Einsch\"atzung des Kaufpreises und der Risiken, bei der Strukturierung des Kaufvertrags und bei der Finanzierung. Gerd Kommer versetzt seine Leserinnen und Leser in die Lage, auf Augenh\"ohe mit Verk\"aufern, Maklern und Banken zu sprechen, um das beste Ergebnis zu erzielen. (Verlagstext)},
  isbn = {978-3-593-51554-0},
  langid = {german},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Kommer - 2022 - Immobilienkauf und Finanzierung für Selbstnutzer Geld sparen und Fehler vermeiden beim Kauf der eig.pdf}
}

@book{kommerSouveraenInvestierenMit2024,
  title = {{Souver\"an investieren mit Indexfonds und ETFs: ein Investmentbuch f\"ur fortgeschrittene Privatanleger}},
  shorttitle = {{Souver\"an investieren mit Indexfonds und ETFs}},
  author = {Kommer, Gerd},
  year = 2024,
  edition = {6., vollst\"andig aktualisierte und \"uberarbeitete Auflage},
  publisher = {Campus Verlag},
  address = {Frankfurt New York},
  isbn = {978-3-593-51770-4},
  langid = {german}
}

@misc{kommerSouveraenInvestierenVor2020,
  title = {{Souver\"an investieren vor und im Ruhestand: mit ETFs Ihren Lebensstandard und Ihre Verm\"ogensziele sichern}},
  shorttitle = {{Souver\"an investieren vor und im Ruhestand}},
  author = {Kommer, Gerd and Pappenberger, Sebastian},
  year = 2020,
  edition = {Ungek\"urzte Lesefassung},
  publisher = {Abod Verlag GmbH},
  address = {M\"unchen},
  isbn = {978-3-593-51245-7 978-3-95471-740-8},
  langid = {german},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Kommer and Pappenberger - 2020 - Souverän investieren vor und im Ruhestand mit ETFs Ihren Lebensstandard und Ihre Vermögensziele sic.pdf}
}

@book{kommerSouveraenVermoegenSchuetzen2021,
  title = {{Souver\"an Verm\"ogen sch\"utzen: wie sich Verm\"ogende gegen Risiken absichern -- ein praktischer Asset-Protection-Ratgeber}},
  shorttitle = {{Souver\"an Verm\"ogen sch\"utzen}},
  author = {Kommer, Gerd and Gierhake, Olaf},
  year = 2021,
  publisher = {Campus Verlag},
  address = {Frankfurt New York},
  abstract = {Verm\"ogen sichern f\"ur die jetzige und die n\"achste Generation, finanziell unabh\"angig sein und bleiben. So lauten die Ziele, die wohlhabende Menschen in der zweiten Lebensh\"alfte bewegen. Gerade heute machen sich viele von ihnen Sorgen um ihr Eigentum. Gerd Kommer und Olaf Gierhake, beide selbst finanziell unabh\"angige Unternehmer, zeigen in diesem Schritt-f\"ur-Schritt-Ratgeber praxisnahe, wirksame Instrumente des Verm\"ogensschutzes. Zur Senkung der Belastung mit Kosten und Steuern. Zur Reduktion politischer, volkswirtschaftlicher, steuerlicher, rechtlicher und marktbedingter Risiken. Einen \"uberzeugenden Mehrwert liefern hier Familienstiftungen IBM und zwar f\"ur alle wichtigen Verm\"ogensarten: liquide Investments, Immobilien und Unternehmensbeteiligungen. (Verlagstext)},
  isbn = {978-3-593-51368-3},
  langid = {german},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Kommer and Gierhake - 2021 - Souverän Vermögen schützen wie sich Vermögende gegen Risiken absichern – ein praktischer Asset-Prot.pdf}
}

@inproceedings{kossenSelfAttentionDatapointsGoing2021,
  title = {Self-Attention between Datapoints: Going beyond Individual Input-Output Pairs in Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
  year = 2021,
  volume = {34},
  pages = {28742--28756},
  publisher = {Curran Associates, Inc.},
  address = {Online}
}

@article{kozlowskiSamplesElectronicInvoices2021,
  title = {Samples of Electronic Invoices},
  author = {Koz{\l}owski, Marek and Weichbroth, Pawe{\l}},
  year = 2021,
  month = jun,
  volume = {2},
  publisher = {Mendeley Data},
  doi = {10.17632/tnj49gpmtz.2},
  urldate = {2024-12-10},
  abstract = {Electronic invoices have become the product of the information age, increasing their utility on the nowadays market. Looking at real electronic invoices across the globe, we have come up with sufficient placement of the information. Each detail has been generated in a programmable way using Python programs. Billing information is minimalistic to omit or lower the chance of fraud detection. The process of collecting each product has been achieved by scrapping popular online marketplaces. As a result, categorized groups have been created to imitate a manner of the persona. The direction of the potential reusability is heading towards becoming an input of the machine learning fraud detection algorithms or data extraction mechanisms. Datasets presents 1000 samples each of auto-generated invoices containing: - valid information. - valid information with colored iban background. RGB color of a background varies between (255,255,240) to (255,255,254). - valid information with modified space between iban characters. Charspace coefficient varies between 0.001 to 1. Both ends of a special invoice modifier represents a domain from detectable to non-detectable factor by a human eye. Nomenclature: invoice\_{$<$}invoice\_id{$>$}(\_charspace\_{$<$}coefficent\_numerator{$>$})(\_color\_B\_{$<$}blue\_color\_value{$>$}).pdf},
  langid = {english},
  file = {/Users/markusbilz/Zotero/storage/JZCP73UX/2.html}
}

@article{kraussDeepNeuralNetworks2017,
  title = {Deep Neural Networks, Gradient-Boosted Trees, Random Forests: Statistical Arbitrage on the {{S}}\&{{P}} 500},
  author = {Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas},
  year = 2017,
  journal = {European Journal of Operational Research},
  volume = {259},
  number = {2},
  pages = {689--702},
  doi = {10.1016/j.ejor.2016.10.031}
}

@article{krogerKapitelOutlierDetection,
  title = {Kapitel 6: Outlier Detection},
  author = {Kr{\"o}ger, Peer and Zimek, Arthur},
  pages = {36}
}

@misc{kudoSentencePieceSimpleLanguage2018,
  title = {{{SentencePiece}}: {{A}} Simple and Language Independent Subword Tokenizer and Detokenizer for {{Neural Text Processing}}},
  shorttitle = {{{SentencePiece}}},
  author = {Kudo, Taku and Richardson, John},
  year = 2018,
  month = aug,
  number = {arXiv:1808.06226},
  eprint = {1808.06226},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.06226},
  urldate = {2024-11-26},
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/ sentencepiece.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/markusbilz/Zotero/storage/JUH8454D/Kudo and Richardson - 2018 - SentencePiece A simple and language independent subword tokenizer and detokenizer for Neural Text P.pdf}
}

@article{kuhlHumanVsSupervised2020,
  title = {Human vs. Supervised Machine Learning: Who Learns Patterns Faster?},
  author = {K{\"u}hl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik},
  year = 2020,
  journal = {arXiv:2012.03661 [cs]},
  eprint = {2012.03661},
  primaryclass = {cs},
  urldate = {2021-11-27},
  archiveprefix = {arXiv}
}

@book{kuhnFeatureEngineeringSelection2020,
  title = {Feature Engineering and Selection: A Practical Approach for Predictive Models},
  author = {Kuhn, Max and Johnson, Kjell},
  year = 2020,
  volume = {74},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, FL}
}

@misc{kusupatiMatryoshkaRepresentationLearning2024,
  title = {Matryoshka {{Representation Learning}}},
  author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and {Howard-Snyder}, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
  year = 2024,
  month = feb,
  number = {arXiv:2205.13147},
  eprint = {2205.13147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.13147},
  urldate = {2024-11-26},
  abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14\texttimes{} smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14\texttimes{} real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2\% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Kusupati et al. - 2024 - Matryoshka Representation Learning.pdf}
}

@misc{kusupatiMatryoshkaRepresentationLearning2024a,
  title = {Matryoshka {{Representation Learning}}},
  author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and {Howard-Snyder}, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
  year = 2024,
  month = feb,
  number = {arXiv:2205.13147},
  eprint = {2205.13147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.13147},
  urldate = {2025-11-12},
  abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14\texttimes{} smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14\texttimes{} real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2\% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{lambertonIntroductionStochasticCalculus,
  title = {Introduction to Stochastic Calculus Applied to Finance, Second Edition},
  author = {Lamberton, Damien and Lapeyre, Bernard},
  pages = {253}
}

@misc{lampleLargeMemoryLayers2019,
  title = {Large Memory Layers with Product Keys},
  author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  year = 2019,
  eprint = {1907.05242},
  urldate = {2023-01-16},
  archiveprefix = {arXiv}
}

@incollection{lecunEfficientBackProp2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = 2012,
  volume = {7700},
  pages = {9--48},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8\_3}
}

@article{leeInferringInvestorBehavior2000,
  title = {Inferring Investor Behavior: Evidence from {{TORQ}} Data},
  author = {Lee, Charles and Radhakrishna, Balkrishna},
  year = 2000,
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {2},
  pages = {83--111},
  doi = {10.1016/S1386-4181(00)00002-1}
}

@article{leeInferringTradeDirection1991,
  title = {Inferring Trade Direction from Intraday Data},
  author = {Lee, Charles and Ready, Mark J.},
  year = 1991,
  journal = {The Journal of Finance},
  volume = {46},
  number = {2},
  pages = {733--746},
  doi = {10.1111/j.1540-6261.1991.tb02683.x}
}

@article{leeInformationContentsOrder2019,
  title = {Information {{Contents}} of {{Order Flow Toxicity}} in the {{Options Market}} : {{The Case}} of {{KOSPI200 Index Options}}},
  author = {Lee, Jaeram},
  year = 2019,
  journal = {Journal of Derivatives and Quantitative Studies},
  volume = {27},
  number = {4},
  pages = {365--400},
  doi = {10.1108/JDQS-04-2019-B0001},
  urldate = {2023-06-20}
}

@article{leeMarketIntegrationPrice1993,
  title = {Market Integration and Price Execution for {{NYSE-listed}} Securities},
  author = {Lee, Charles},
  year = 1993,
  journal = {The Journal of Finance},
  volume = {48},
  number = {3},
  pages = {1009--1038},
  doi = {10.1111/j.1540-6261.1993.tb04028.x},
  urldate = {2023-02-25}
}

@article{leePseudolabelSimpleEfficient,
  title = {Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
  author = {Lee, Dong-Hyun},
  year = 2013,
  pages = {7}
}

@inproceedings{leeSetTransformerFramework2019,
  title = {Set Transformer: A Framework for Attention-Based Permutation-Invariant Neural Networks},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  year = 2019,
  pages = {3744--3753},
  publisher = {PMLR},
  urldate = {2023-04-20}
}

@inproceedings{lemorvanWhatGoodImputation2021,
  title = {What's a Good Imputation to Predict with Missing Values?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Le Morvan, Marine and Josse, Julie and Scornet, Erwan and Varoquaux, Gael},
  year = 2021,
  volume = {34},
  pages = {11530--11540},
  publisher = {Curran Associates, Inc.}
}

@misc{levinTransferLearningDeep2022,
  title = {Transfer Learning with Deep Tabular Models},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = 2022,
  eprint = {2206.15306},
  archiveprefix = {arXiv}
}

@inproceedings{liangFactorizationMeetsItem2016,
  title = {Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-Occurrence},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Liang, Dawen and Altosaar, Jaan and Charlin, Laurent and Blei, David M.},
  year = 2016,
  pages = {59--66},
  publisher = {ACM},
  address = {Boston Massachusetts USA},
  doi = {10.1145/2959100.2959182},
  urldate = {2021-05-04}
}

@misc{liDebiasedMDIFeature2019,
  title = {A {{Debiased MDI Feature Importance Measure}} for {{Random Forests}}},
  author = {Li, Xiao and Wang, Yu and Basu, Sumanta and Kumbier, Karl and Yu, Bin},
  year = 2019,
  month = oct,
  number = {arXiv:1906.10845},
  eprint = {1906.10845},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.10845},
  urldate = {2025-11-12},
  abstract = {Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al. (3) for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a new MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Li et al. - 2019 - A Debiased MDI Feature Importance Measure for Random Forests.pdf}
}

@article{liINVESTABLEINTERPRETABLEMACHINE,
  title = {Investable and Interpretable Machine Learning for Equities},
  author = {Li, Yimou and Simon, Zachary and Turkington, David},
  pages = {34}
}

@article{Lillo_2003,
  title = {Econophysics: Master Curve for Price-Impact Function.},
  author = {Lillo, Fabrizio and Farmer, J. Doyne and Mantegna, Rosario N.},
  year = 2003,
  journal = {Nature},
  doi = {10.1038/421129a},
  mag_id = {2078237894},
  pmcid = {null},
  pmid = {12520292}
}

@inproceedings{lin-2004-rouge,
  title = {{{ROUGE}}: A Package for Automatic Evaluation of Summaries},
  booktitle = {Text Summarization Branches Out},
  author = {Lin, Chin-Yew},
  year = 2004,
  month = jul,
  pages = {74--81},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain}
}

@article{linnainmaaHistoryCrossSection,
  title = {The History of the Cross Section of Stock Returns},
  author = {Linnainmaa, Juhani T and Roberts, Michael},
  pages = {69}
}

@article{linnainmaaWeatherTimeSeries2009,
  title = {Weather and Time Series Determinants of Liquidity in a Limit Order Market},
  author = {Linnainmaa, Juhani T. and Rosu, Ioanid},
  year = 2009,
  journal = {null},
  doi = {10.2139/ssrn.1108862},
  mag_id = {1595367183},
  pmcid = {null},
  pmid = {null}
}

@misc{linSurveyTransformers2021,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  year = 2021,
  eprint = {2106.04554},
  urldate = {2022-12-04},
  archiveprefix = {arXiv}
}

@article{linWhyOptionsPrices2015,
  title = {Why Do Options Prices Predict Stock Returns? {{Evidence}} from Analyst Tipping},
  author = {Lin, Tse-Chun and Lu, Xiaolong},
  year = 2015,
  journal = {Journal of Banking \& Finance},
  volume = {52},
  pages = {17--28},
  doi = {10.1016/j.jbankfin.2014.11.008},
  urldate = {2022-07-12}
}

@article{liptonMythosModelInterpretability2017,
  title = {The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  author = {Lipton, Zachary C.},
  year = 2018,
  journal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  doi = {10.1145/3236386.3241340}
}

@article{littleStatisticalAnalysisMissing,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick J A and {Rubin, Donald}},
  doi = {10.1002/9781119013563}
}

@article{littlestoneWeightedMajorityAlgorithm,
  title = {The Weighted Majority Algorithm},
  author = {Littlestone, Nick and Warmuth, Manfred K},
  journal = {. Introduction},
  pages = {39}
}

@article{liuDataQualityProblems2020,
  title = {Data Quality Problems Troubling Business and Financial Researchers: A Literature Review and Synthetic Analysis},
  author = {Liu, Grace},
  year = 2020,
  journal = {Journal of Business \& Finance Librarianship},
  volume = {25},
  number = {3-4},
  pages = {315--371},
  doi = {10.1080/08963568.2020.1847555},
  urldate = {2021-10-29}
}

@misc{liuImprovedBaselinesVisual2024,
  title = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  year = 2024,
  month = may,
  number = {arXiv:2310.03744},
  eprint = {2310.03744},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-20},
  abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {$\sim$}1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/markusbilz/Zotero/storage/KIY8MB3G/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf}
}

@misc{liuMonolithRealTime2022,
  title = {Monolith: Real Time Recommendation System with Collisionless Embedding Table},
  author = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
  year = 2022,
  eprint = {2209.07663},
  urldate = {2022-11-01},
  archiveprefix = {arXiv}
}

@misc{liuPayAttentionMlps2021,
  title = {Pay Attention to Mlps},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  year = 2021,
  eprint = {2105.08050},
  urldate = {2022-12-07},
  archiveprefix = {arXiv}
}

@inproceedings{liuRelatedPinsPinterest2017,
  title = {Related Pins at Pinterest: The Evolution of a Real-World Recommender System},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web Companion}} - {{WWW}} '17 {{Companion}}},
  author = {Liu, David C. and Rogers, Stephanie and Shiau, Raymond and Kislyuk, Dmitry and Ma, Kevin C. and Zhong, Zhigang and Liu, Jenny and Jing, Yushi},
  year = 2017,
  pages = {583--592},
  publisher = {ACM Press},
  address = {Perth, Australia},
  doi = {10.1145/3041021.3054202}
}

@inproceedings{liuRethinkingSkipConnection2020,
  title = {Rethinking Skip Connection with Layer Normalization},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Liu, Fenglin and Ren, Xuancheng and Zhang, Zhiyuan and Sun, Xu and Zou, Yuexian},
  year = 2020,
  pages = {3586--3598},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.320}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {Roberta: A Robustly Optimized Bert Pretraining Approach},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = 2019,
  eprint = {1907.11692},
  urldate = {2023-01-13},
  archiveprefix = {arXiv}
}

@inproceedings{liuSTAMPShorttermAttention2018,
  title = {{{STAMP}}: Short-Term Attention/Memory Priority Model for Session-Based Recommendation},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Qiao and Zeng, Yifu and Mokhosi, Refuoe and Zhang, Haibin},
  year = 2018,
  pages = {1831--1839},
  publisher = {ACM},
  address = {London United Kingdom},
  doi = {10.1145/3219819.3219950},
  urldate = {2021-05-04}
}

@inproceedings{liuUnderstandingDifficultyTraining2020,
  title = {Understanding the Difficulty of Training Transformers},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  year = 2020,
  pages = {5747--5763},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.463}
}

@misc{liuVarianceAdaptiveLearning2021,
  title = {On the Variance of the Adaptive Learning Rate and Beyond},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  year = 2021,
  eprint = {1908.03265},
  urldate = {2023-01-09},
  archiveprefix = {arXiv}
}

@article{Lo_2002,
  title = {Econometric Models of Limit-Order Executions},
  author = {Lo, Andrew W. and MacKinlay, A. Craig and Zhang, June},
  year = 2002,
  journal = {Journal of Financial Economics},
  doi = {10.1016/s0304-405x(02)00134-4},
  mag_id = {2041147733},
  pmcid = {null},
  pmid = {null}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  author = {Lones, Michael A.},
  year = 2022,
  eprint = {2108.02497},
  urldate = {2022-11-06},
  archiveprefix = {arXiv}
}

@book{lopezdepradoAdvancesFinancialMachine2018,
  title = {Advances in Financial Machine Learning},
  author = {{L{\'o}pez de Prado}, Marcos},
  year = 2018,
  publisher = {Wiley},
  address = {Hoboken, NJ, USA}
}

@inproceedings{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled Weight Decay Regularization},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = 2019,
  address = {New Orleans, LA, USA}
}

@misc{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: Stochastic Gradient Descent with Warm Restarts},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = 2017,
  eprint = {1608.03983},
  archiveprefix = {arXiv}
}

@article{ludewigEvaluationSessionbasedRecommendation2018,
  title = {Evaluation of Session-Based Recommendation Algorithms},
  author = {Ludewig, Malte and Jannach, Dietmar},
  year = 2018,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {28},
  number = {4-5},
  eprint = {1803.09587},
  pages = {331--390},
  doi = {10.1007/s11257-018-9209-6},
  urldate = {2021-04-22},
  archiveprefix = {arXiv}
}

@misc{lundbergConsistentIndividualizedFeature2019,
  title = {Consistent Individualized Feature Attribution for Tree Ensembles},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  year = 2019,
  eprint = {1802.03888},
  archiveprefix = {arXiv}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M. and Lee, Su-In},
  year = 2017,
  series = {{{NeurIPS}} 2017},
  volume = {31},
  pages = {4768--4777},
  publisher = {Curran Associates, Inc.},
  address = {Long Beach, CA}
}

@inproceedings{luoCollaborativeSelfattentionNetwork2020,
  title = {Collaborative Self-Attention Network for Session-Based Recommendation},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Anjing and Zhao, Pengpeng and Liu, Yanchi and Zhuang, Fuzhen and Wang, Deqing and Xu, Jiajie and Fang, Junhua and Sheng, Victor S.},
  year = 2020,
  pages = {2591--2597},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Yokohama, Japan},
  doi = {10.24963/ijcai.2020/359},
  urldate = {2021-04-19}
}

@misc{MachineLearningHow,
  title = {Machine Learning - How to Intuitively Explain What a Kernel Is?},
  urldate = {2021-08-14}
}

@misc{maEra1bitLLMs2024,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  year = 2024,
  month = feb,
  number = {arXiv:2402.17764},
  eprint = {2402.17764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17764},
  urldate = {2024-11-28},
  abstract = {Recent research, such as BitNet [WMD+23], is paving the way for a new era of 1bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \textbraceleft -1, 0, 1\textbraceright. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Ma et al. - 2024 - The Era of 1-bit LLMs All Large Language Models are in 1.58 Bits.pdf}
}

@misc{malininUncertaintyGradientBoosting2021,
  title = {Uncertainty in Gradient Boosting via Ensembles},
  author = {Malinin, Andrey and Prokhorenkova, Liudmila and Ustimenko, Aleksei},
  year = 2021,
  eprint = {2006.10562},
  archiveprefix = {arXiv}
}

@article{mallapragadaSemiBoostBoostingSemiSupervised2009,
  title = {{{SemiBoost}}: Boosting for Semi-Supervised Learning},
  author = {Mallapragada, P.K. and {Rong Jin} and Jain, A.K. and {Yi Liu}},
  year = 2009,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {31},
  number = {11},
  pages = {2000--2014},
  doi = {10.1109/TPAMI.2008.235},
  urldate = {2023-04-23}
}

@article{Manaster_1982,
  title = {Option Prices as Predictors of Equilibrium Stock Prices},
  author = {Manaster, Steven and Rendleman, Richard J.},
  year = 1982,
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1982.tb03597.x},
  mag_id = {2096808000},
  pmcid = {null},
  pmid = {null}
}

@article{mankowitzFasterSortingAlgorithms,
  title = {Faster Sorting Algorithms Discovered Using Deep Reinforcement Learning},
  author = {Mankowitz, Daniel J}
}

@article{maraisDeepLearningTabular,
  title = {Deep Learning for Tabular Data: An Exploratory Study},
  author = {Marais, Jan Andr{\'e}},
  pages = {144}
}

@book{martinEconometricModellingTime2012,
  title = {Econometric Modelling with Time Series: Specification, Estimation and Testing},
  author = {Martin, Vance and Hurn, Stan and Harris, David},
  year = 2012,
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139043205},
  urldate = {2020-12-12}
}

@techreport{martinMarketEfficiencyAge2019,
  title = {Market Efficiency in the Age of Big Data},
  author = {Martin, Ian and Nagel, Stefan},
  year = 2019,
  number = {w26586},
  pages = {w26586},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w26586},
  urldate = {2021-11-03}
}

@article{martinWhatExpectedReturn2016,
  title = {What Is the Expected Return on the Market?},
  author = {Martin, Ian},
  year = 2016,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2772101}
}

@article{martinWhatExpectedReturn2019,
  title = {What Is the Expected Return on a Stock?},
  author = {Martin, Ian and Wagner, Christian},
  year = 2019,
  journal = {The Journal of Finance},
  volume = {74},
  number = {4},
  pages = {1887--1929},
  doi = {10.1111/jofi.12778}
}

@article{matthewsComparisonPredictedObserved1975,
  title = {Comparison of the Predicted and Observed Secondary Structure of {{T4}} Phage Lysozyme},
  author = {Matthews, B.W.},
  year = 1975,
  journal = {Biochimica et Biophysica Acta (BBA) - Protein Structure},
  volume = {405},
  number = {2},
  pages = {442--451},
  doi = {10.1016/0005-2795(75)90109-9},
  urldate = {2023-03-12}
}

@article{mayhewCompetitionMarketStructure2002,
  title = {Competition, {{Market Structure}}, and {{Bid-Ask Spreads}} in {{Stock Option Markets}}},
  author = {Mayhew, Stewart},
  year = 2002,
  journal = {The Journal of Finance},
  volume = {57},
  number = {2},
  pages = {931--958},
  doi = {10.1111/1540-6261.00447},
  urldate = {2023-06-22}
}

@misc{mccoyBERTsFeatherNot2020,
  title = {{{BERTs}} of a Feather Do Not Generalize Together: {{Large}} Variability in Generalization across Models with Similar Test Set Performance},
  author = {McCoy, R. Thomas and Min, Junghyun and Linzen, Tal},
  year = 2020,
  eprint = {1911.02969},
  urldate = {2023-07-04},
  archiveprefix = {arXiv}
}

@article{mcnemarNoteSamplingError1947,
  title = {Note on the Sampling Error of the Difference between Correlated Proportions or Percentages},
  author = {McNemar, Quinn},
  year = 1947,
  journal = {Psychometrika},
  volume = {12},
  number = {2},
  pages = {153--157},
  doi = {10.1007/BF02295996},
  urldate = {2023-06-11}
}

@article{measeBoostedClassificationTrees,
  title = {Boosted Classification Trees and Class Probability/Quantile Estimation},
  author = {Mease, David and Wyner, Abraham J and Buja, Andreas},
  pages = {31}
}

@misc{melisStateArtEvaluation2017,
  title = {On the State of the Art of Evaluation in Neural Language Models},
  author = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  year = 2017,
  eprint = {1707.05589},
  urldate = {2022-10-26},
  archiveprefix = {arXiv}
}

@inproceedings{merchantWhatHappensBERT2020,
  title = {What Happens to {{BERT}} Embeddings during Fine-Tuning?},
  booktitle = {Proceedings of the Third {{BlackboxNLP}} Workshop on Analyzing and Interpreting Neural Networks for {{NLP}}},
  author = {Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
  year = 2020,
  pages = {33--44},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.blackboxnlp-1.4}
}

@misc{meyesAblationStudiesArtificial2019,
  title = {Ablation Studies in Artificial Neural Networks},
  author = {Meyes, Richard and Lu, Melanie and {de Puiseau}, Constantin Waubert and Meisen, Tobias},
  year = 2019,
  eprint = {1901.08644},
  urldate = {2022-12-15},
  archiveprefix = {arXiv}
}

@inproceedings{michelAreSixteenHeads2019,
  title = {Are Sixteen Heads Really Better than One?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year = 2019,
  volume = {32},
  pages = {14014--14024},
  publisher = {Curran Associates, Inc.},
  address = {Vancouver, Canada}
}

@inproceedings{mikolovLinguisticRegularitiesContinuous2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = 2013,
  pages = {746--751},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, GE, USA}
}

@misc{millerAddingErrorBars2024,
  title = {Adding {{Error Bars}} to {{Evals}}: {{A Statistical Approach}} to {{Language Model Evaluations}}},
  shorttitle = {Adding {{Error Bars}} to {{Evals}}},
  author = {Miller, Evan},
  year = 2024,
  month = nov,
  number = {arXiv:2411.00640},
  eprint = {2411.00640},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00640},
  urldate = {2025-11-17},
  abstract = {Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Statistics - Applications},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Miller - 2024 - Adding Error Bars to Evals A Statistical Approach to Language Model Evaluations.pdf}
}

@misc{mirzaeiHowUseDeepLearning2019,
  title = {How to Use Deep-Learning for Feature-Selection, Python, Keras},
  author = {Mirzaei, Ali},
  year = 2019,
  urldate = {2021-11-05}
}

@article{mockusApplicationBayesianApproach1994,
  title = {Application of Bayesian Approach to Numerical Methods of Global and Stochastic Optimization},
  author = {Mockus, Jonas},
  year = 1994,
  journal = {Journal of Global Optimization},
  volume = {4},
  number = {4},
  pages = {347--365},
  doi = {10.1007/BF01099263}
}

@misc{modelerSHAPNotAll2023,
  type = {Substack Newsletter},
  title = {{{SHAP}} Is Not All You Need},
  author = {Modeler, Mindful},
  year = 2023,
  urldate = {2023-03-26}
}

@article{mogharStockMarketPrediction2020,
  title = {Stock Market Prediction Using {{LSTM}} Recurrent Neural Network},
  author = {Moghar, Adil and Hamiche, Mhamed},
  year = 2020,
  journal = {Procedia Computer Science},
  volume = {170},
  pages = {1168--1173},
  doi = {10.1016/j.procs.2020.03.049},
  urldate = {2022-07-12}
}

@misc{molnarRelatingPartialDependence2021,
  title = {Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process},
  author = {Molnar, Christoph and Freiesleben, Timo and K{\"o}nig, Gunnar and Casalicchio, Giuseppe and Wright, Marvin N. and Bischl, Bernd},
  year = 2021,
  eprint = {2109.01433},
  urldate = {2023-02-08},
  archiveprefix = {arXiv}
}

@article{muravyevOptionsTradingCosts2020,
  title = {Options Trading Costs Are Lower than You Think},
  author = {Muravyev, Dmitriy and Pearson, Neil D},
  editor = {Van Nieuwerburgh, Stijn},
  year = 2020,
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {11},
  pages = {4973--5014},
  doi = {10.1093/rfs/hhaa010}
}

@article{muravyevOrderFlowExpected2016,
  title = {Order Flow and Expected Option Returns: Order Flow and Expected Option Returns},
  author = {Muravyev, Dmitriy},
  year = 2016,
  journal = {The Journal of Finance},
  volume = {71},
  number = {2},
  pages = {673--708},
  doi = {10.1111/jofi.12380}
}

@article{muravyevTherePriceDiscovery2013,
  title = {Is There Price Discovery in Equity Options?},
  author = {Muravyev, Dmitriy and Pearson, Neil D. and Paul Broussard, John},
  year = 2013,
  journal = {Journal of Financial Economics},
  volume = {107},
  number = {2},
  pages = {259--283},
  doi = {10.1016/j.jfineco.2012.09.003},
  urldate = {2023-06-26}
}

@article{nabiNovelApproachStock2020,
  title = {A Novel Approach for Stock Price Prediction Using Gradient Boosting Machine with Feature Engineering ({{GBM-wFE}})},
  author = {Nabi, Rebwar M. and Ab. M. Saeed, Soran and Harron, Habibollah},
  year = 2020,
  journal = {Kurdistan Journal of Applied Research},
  volume = {5},
  number = {1},
  pages = {28--48},
  doi = {10.24017/science.2020.1.3},
  urldate = {2022-05-06}
}

@book{nagelMachineLearningAsset2021,
  title = {Machine Learning in Asset Pricing},
  author = {Nagel, Stefan},
  year = 2021
}

@misc{narangTransformerModificationsTransfer2021,
  title = {Do Transformer Modifications Transfer across Implementations and Applications?},
  author = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yanqi and Li, Wei and Ding, Nan and Marcus, Jake and Roberts, Adam and Raffel, Colin},
  year = 2021,
  eprint = {2102.11972},
  archiveprefix = {arXiv}
}

@misc{nasdaqincFrequentlyAskedQuestions2017,
  title = {Frequently Asked Questions Ise Open/Close Trade Profile Gemx Open/Close Trade Profile},
  author = {{NASDAQ Inc.}},
  year = 2017,
  urldate = {2023-03-03}
}

@misc{nassarSmolDoclingUltracompactVisionlanguage2025,
  title = {{{SmolDocling}}: {{An}} Ultra-Compact Vision-Language Model for End-to-End Multi-Modal Document Conversion},
  shorttitle = {{{SmolDocling}}},
  author = {Nassar, Ahmed and Marafioti, Andres and Omenetti, Matteo and Lysak, Maksym and Livathinos, Nikolaos and Auer, Christoph and Morin, Lucas and de Lima, Rafael Teixeira and Kim, Yusik and Gurbuz, A. Said and Dolfi, Michele and Farr{\'e}, Miquel and Staar, Peter W. J.},
  year = 2025,
  month = mar,
  number = {arXiv:2503.11576},
  eprint = {2503.11576},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.11576},
  urldate = {2025-04-13},
  abstract = {We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms --- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Nassar et al. - 2025 - SmolDocling An ultra-compact vision-language model for end-to-end multi-modal document conversion.pdf}
}

@incollection{nelsonMachineLearningStrategic2023,
  title = {Machine Learning for Strategic Trade Analysis},
  booktitle = {Methods of {{Strategic Trade Analysis}}: {{Data-Driven Approaches}} to {{Detect Illicit Dual-Use Trade}}},
  author = {Nelson, Christopher},
  editor = {Nelson, Christopher},
  year = 2023,
  series = {Advanced {{Sciences}} and {{Technologies}} for {{Security Applications}}},
  pages = {113--146},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-20036-6\_10},
  urldate = {2022-12-29}
}

@misc{NetflixUpdateTry,
  title = {Netflix Update: Try This at Home},
  urldate = {2021-04-20}
}

@incollection{neumannMotivatingSupportingUser2007,
  title = {Motivating and Supporting User Interaction with Recommender Systems},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Neumann, Andreas W.},
  editor = {Kov{\'a}cs, L{\'a}szl{\'o} and Fuhr, Norbert and Meghini, Carlo},
  year = 2007,
  volume = {4675},
  pages = {428--439},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74851-9\_36},
  urldate = {2021-03-21}
}

@inproceedings{NEURIPS2024_2ad2dffb,
  title = {Be like a Goldfish, Dont Memorize! {{Mitigating}} Memorization in Generative Llms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hans, Abhimanyu and Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Kazemi, Hamid and Singhania, Prajwal and Singh, Siddharth and Somepalli, Gowthami and Geiping, Jonas and Bhatele, Abhinav and Goldstein, Tom},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = 2024,
  volume = {37},
  pages = {24022--24045},
  publisher = {Curran Associates, Inc.},
  doi = {10.52202/079017-0757}
}

@inproceedings{ngFeatureSelectionVs2004,
  title = {Feature Selection, \${{L}}\_1\$ vs. \${{L}}\_2\$ Regularization, and Rotational Invariance},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning - {{ICML}} '04},
  author = {Ng, Andrew Y.},
  year = 2004,
  pages = {78},
  publisher = {ACM Press},
  address = {Banff, Alberta, Canada},
  doi = {10.1145/1015330.1015435},
  urldate = {2023-01-28}
}

@misc{nguyenTransformersTearsImproving2019,
  title = {Transformers without Tears: Improving the Normalization of Self-Attention},
  author = {Nguyen, Toan Q. and Salazar, Julian},
  year = 2019,
  eprint = {1910.05895},
  doi = {10.5281/zenodo.3525484},
  urldate = {2023-01-17},
  archiveprefix = {arXiv}
}

@article{niDoesOptionTrading2021,
  title = {Does {{Option Trading Have}} a {{Pervasive Impact}} on {{Underlying Stock Prices}}?},
  author = {Ni, Sophie X and Pearson, Neil D and Poteshman, Allen M and White, Joshua},
  editor = {Karolyi, Andrew},
  year = 2021,
  journal = {The Review of Financial Studies},
  volume = {34},
  number = {4},
  pages = {1952--1986},
  doi = {10.1093/rfs/hhaa082},
  urldate = {2023-06-23}
}

@article{ningShapleyVariableImportance2022,
  title = {Shapley Variable Importance Cloud for Interpretable Machine Learning},
  author = {Ning, Yilin and Ong, Marcus Eng Hock and Chakraborty, Bibhas and Goldstein, Benjamin Alan and Ting, Daniel Shu Wei and Vaughan, Roger and Liu, Nan},
  year = 2022,
  journal = {Patterns},
  volume = {3},
  number = {4},
  pages = {100452},
  doi = {10.1016/j.patter.2022.100452},
  urldate = {2023-05-22}
}

@book{nisbetHandbookStatisticalAnalysis2009,
  title = {Handbook of Statistical Analysis and Data Mining Applications},
  author = {Nisbet, Robert and Elder, John F. and Miner, Gary},
  year = 2009,
  publisher = {Academic Press/Elsevier},
  address = {Amsterdam ; Boston}
}

@article{niVolatilityInformationTrading2008,
  title = {Volatility {{Information Trading}} in the {{Option Market}}},
  author = {Ni, Sophie X. and Pan, Jun and Poteshman, Allen M.},
  year = 2008,
  journal = {The Journal of Finance},
  volume = {63},
  number = {3},
  pages = {1059--1091},
  doi = {10.1111/j.1540-6261.2008.01352.x},
  urldate = {2023-06-20}
}

@misc{noriAccuracyInterpretabilityDifferential2021,
  title = {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},
  author = {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},
  year = 2021,
  eprint = {2106.09680},
  urldate = {2022-12-03},
  archiveprefix = {arXiv}
}

@inproceedings{nothmanStopWordLists2018,
  title = {Stop Word Lists in Free Open-Source Software Packages},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  year = 2018,
  pages = {7--12},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/W18-2502},
  urldate = {2021-04-24}
}

@incollection{nowakAccuracyTradeClassification2020,
  title = {The Accuracy of Trade Classification Rules for the Selected {{CEE}} Stock Exchanges},
  booktitle = {Contemporary {{Trends}} and {{Challenges}} in {{Finance}}},
  author = {Nowak, Sabina},
  editor = {Jajuga, Krzysztof and {Locarek-Junge}, Hermann and Orlowski, Lucjan T. and Staehr, Karsten},
  year = 2020,
  pages = {65--75},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-43078-8\_6},
  urldate = {2023-02-01}
}

@misc{ntakourisTimeSeriesTransformer2021,
  title = {The Time Series Transformer},
  author = {Ntakouris, Theodoros},
  year = 2021,
  urldate = {2021-11-06}
}

@misc{Ochama,
  title = {Ochama},
  urldate = {2023-02-17}
}

@article{odders-whiteOccurrenceConsequencesInaccurate2000,
  title = {On the Occurrence and Consequences of Inaccurate Trade Classification},
  author = {{Odders-White}, Elizabeth R},
  year = 2000,
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {3},
  pages = {259--286},
  doi = {10.1016/S1386-4181(00)00006-9}
}

@article{olbrysEvaluatingTradeSide2018,
  title = {Evaluating Trade Side Classification Algorithms Using Intraday Data from the Warsaw Stock Exchange},
  author = {Olbrys, Joanna and Mursztyn, Micha{\l}},
  year = 2018,
  publisher = {Karlsruhe},
  doi = {10.5445/KSP/1000085951/20},
  urldate = {2022-10-03},
  copyright = {Closed Access, Creative Commons Namensnennung -- Weitergabe unter gleichen Bedingungen 4.0 International}
}

@misc{oliverRealisticEvaluationDeep2019,
  title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  year = 2019,
  eprint = {1804.09170},
  urldate = {2022-10-30},
  archiveprefix = {arXiv}
}

@misc{OptionTrades,
  title = {Option Trades},
  urldate = {2022-11-08}
}

@book{owenHyperparameterTuningPython2022,
  title = {Hyperparameter Tuning with Python},
  author = {Owen, Louis},
  year = 2022,
  publisher = {PACKT PUBLISHING LIMITED},
  address = {S.l.}
}

@article{panayidesBulkVolumeClassification2019,
  title = {Bulk Volume Classification and Information Detection},
  author = {Panayides, Marios A. and Shohfi, Thomas D. and Smith, Jared D.},
  year = 2019,
  journal = {Journal of Banking \& Finance},
  volume = {103},
  pages = {113--129},
  doi = {10.1016/j.jbankfin.2019.04.001}
}

@article{panayidesComparingTradeFlow2014,
  title = {Comparing Trade Flow Classification Algorithms in the Electronic Era: The Good, the Bad, and the Uninformative},
  author = {Panayides, Marios A. and Shohfi, Thomas and Smith, Jared D.},
  year = 2014,
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2503628}
}

@article{panInformationOptionVolume2006,
  title = {The Information in Option Volume for Future Stock Prices},
  author = {Pan, Jun and Poteshman, Allen M.},
  year = 2006,
  journal = {Review of Financial Studies},
  volume = {19},
  number = {3},
  pages = {871--908},
  doi = {10.1093/rfs/hhj024}
}

@inproceedings{parmarImageTransformer2018,
  title = {Image Transformer},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = 2018,
  pages = {4055--4064},
  publisher = {PMLR}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = 2019,
  series = {{{NeurIPS}} 2019},
  volume = {32},
  pages = {8024--8035},
  publisher = {Curran Associates, Inc.},
  address = {Red Hook, NY}
}

@misc{patrignaniWhyShouldAnyone2021,
  title = {Why Should Anyone Use Colours? Or, Syntax Highlighting beyond Code Snippets},
  author = {Patrignani, Marco},
  year = 2021,
  eprint = {2001.11334},
  urldate = {2022-12-21},
  archiveprefix = {arXiv}
}

@misc{pedregosaScikitlearnMachineLearning2018,
  title = {Scikit-Learn: Machine Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and M{\"u}ller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = 2018,
  eprint = {1201.0490},
  urldate = {2022-10-13},
  archiveprefix = {arXiv}
}

@article{perez-lebelBenchmarkingMissingvaluesApproaches2022,
  title = {Benchmarking Missing-Values Approaches for Predictive Models on Health Databases},
  author = {{Perez-Lebel}, Alexandre and Varoquaux, Ga{\"e}l and Le Morvan, Marine and Josse, Julie and Poline, Jean-Baptiste},
  year = 2022,
  journal = {GigaScience},
  volume = {11},
  pages = {giac013},
  doi = {10.1093/gigascience/giac013}
}

@article{perlinPerformanceTickTest2014,
  title = {On the Performance of the Tick Test},
  author = {Perlin, Marcelo and Brooks, Chris and Dufour, Alfonso},
  year = 2014,
  journal = {The Quarterly Review of Economics and Finance},
  volume = {54},
  number = {1},
  pages = {42--50},
  doi = {10.1016/j.qref.2013.07.009}
}

@article{petersenMatrixCookbook,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72}
}

@article{petersenPostedEffectiveSpreads1994,
  title = {Posted versus Effective Spreads},
  author = {Petersen, Mitchell A. and Fialkowski, David},
  year = 1994,
  journal = {Journal of Financial Economics},
  volume = {35},
  number = {3},
  pages = {269--292},
  doi = {10.1016/0304-405X(94)90034-5},
  urldate = {2023-02-24}
}

@article{petersonEvaluationBiasesExecution2003,
  title = {Evaluation of the Biases in Execution Cost Estimation Using Trade and Quote Data},
  author = {Peterson, Mark and Sirri, Erik},
  year = 2003,
  journal = {Journal of Financial Markets},
  volume = {6},
  number = {3},
  pages = {259--280},
  doi = {10.1016/S1386-4181(02)00065-4}
}

@misc{petersTuneNotTune2019,
  title = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  year = 2019,
  eprint = {1903.05987},
  urldate = {2023-07-04},
  archiveprefix = {arXiv}
}

@incollection{petitUNetTransformerSelf2021,
  title = {U-Net Transformer: Self and Cross Attention for Medical Image Segmentation},
  booktitle = {Machine {{Learning}} in {{Medical Imaging}}},
  author = {Petit, Olivier and Thome, Nicolas and Rambour, Clement and Themyr, Loic and Collins, Toby and Soler, Luc},
  editor = {Lian, Chunfeng and Cao, Xiaohuan and Rekik, Islem and Xu, Xuanang and Yan, Pingkun},
  year = 2021,
  volume = {12966},
  pages = {267--276},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87589-3\_28},
  urldate = {2023-01-20}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  title = {Formal Algorithms for Transformers},
  author = {Phuong, Mary and Hutter, Marcus},
  year = 2022,
  eprint = {2207.09238},
  archiveprefix = {arXiv}
}

@article{Piwowar_2006,
  title = {The Sensitivity of Effective Spread Estimates to Trade--Quote Matching Algorithms},
  author = {Piwowar, Michael S. and Wei, Li},
  year = 2006,
  journal = {Electronic Markets},
  doi = {10.1080/10196780600643803}
}

@article{popelTrainingTipsTransformer2018,
  title = {Training Tips for the Transformer Model},
  author = {Popel, Martin and Bojar, Ond{\v r}ej},
  year = 2018,
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {110},
  number = {1},
  eprint = {1804.00247},
  pages = {43--70},
  doi = {10.2478/pralin-2018-0002},
  urldate = {2022-12-31},
  archiveprefix = {arXiv}
}

@misc{popovNeuralObliviousDecision2019,
  title = {Neural {{Oblivious Decision Ensembles}} for {{Deep Learning}} on {{Tabular Data}}},
  author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  year = 2019,
  eprint = {1909.06312},
  urldate = {2023-05-15},
  archiveprefix = {arXiv}
}

@article{poppeSensitivityVPINChoice2016,
  title = {The Sensitivity of Vpin to the Choice of Trade Classification Algorithm},
  author = {P{\"o}ppe, Thomas and Moos, Sebastian and Schiereck, Dirk},
  year = 2016,
  journal = {Journal of Banking \& Finance},
  volume = {73},
  pages = {165--181},
  doi = {10.1016/j.jbankfin.2016.08.006}
}

@article{porterProbabilityTradeAsk1992,
  title = {The Probability of a Trade at the Ask: An Examination of Interday and Intraday Behavior},
  author = {Porter, David C.},
  year = 1992,
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {27},
  number = {2},
  pages = {209},
  doi = {10.2307/2331368},
  urldate = {2023-02-01},
  jstor = {2331368}
}

@article{Potters_2003,
  title = {More Statistical Properties of Order Books and Price Impact},
  author = {Potters, Marc and Bouchaud, Jean-Philippe and Bouchaud, Jean-Philippe and Bouchaud, Jean-Philippe},
  year = 2003,
  journal = {Physica A-statistical Mechanics and Its Applications},
  doi = {10.1016/s0378-4371(02)01896-4},
  mag_id = {3122247321},
  pmcid = {null},
  pmid = {null}
}

@misc{powerGrokkingGeneralizationOverfitting2022,
  title = {Grokking: Generalization beyond Overfitting on Small Algorithmic Datasets},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = 2022,
  eprint = {2201.02177},
  urldate = {2023-03-05},
  archiveprefix = {arXiv}
}

@misc{pressImprovingTransformerModels2020,
  title = {Improving Transformer Models by Reordering Their Sublayers},
  author = {Press, Ofir and Smith, Noah A. and Levy, Omer},
  year = 2020,
  eprint = {1911.03864},
  urldate = {2023-01-10},
  archiveprefix = {arXiv}
}

@inproceedings{prokhorenkovaCatBoostUnbiasedBoosting2018,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  editor = {{Bengio, Samy} and {Wallach, Hanna M.} and {Grauman, Kristen} and {Larochelle, Hugo} and {Cesa-Bianchi, Nicol\`o} and {Garnett, Roman}},
  year = 2018,
  series = {{{NeurIPS}} 2018},
  volume = {32},
  pages = {6639--6649},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY}
}

@article{prollochsNegationScopeDetection2020,
  title = {Negation Scope Detection for Sentiment Analysis: A Reinforcement Learning Framework for Replicating Human Interpretations},
  author = {Pr{\"o}llochs, Nicolas and Feuerriegel, Stefan and Lutz, Bernhard and Neumann, Dirk},
  year = 2020,
  journal = {Information Sciences},
  volume = {536},
  pages = {205--221},
  doi = {10.1016/j.ins.2020.05.022},
  urldate = {2023-05-01}
}

@article{provostTreeInductionProbabilityBased,
  title = {Tree Induction for Probability-Based Ranking},
  author = {Provost, Foster},
  pages = {17}
}

@misc{pulugundlaAttentionbasedNeuralBeamforming2021,
  title = {Attention-Based Neural Beamforming Layers for Multi-Channel Speech Recognition},
  author = {Pulugundla, Bhargav and Gao, Yang and King, Brian and Keskin, Gokce and Mallidi, Harish and Wu, Minhua and Droppo, Jasha and Maas, Roland},
  year = 2021,
  eprint = {2105.05920},
  urldate = {2023-01-16},
  archiveprefix = {arXiv}
}

@misc{qinScalingLawsSynthetic2025,
  title = {Scaling {{Laws}} of {{Synthetic Data}} for {{Language Models}}},
  author = {Qin, Zeyu and Dong, Qingxiu and Zhang, Xingxing and Dong, Li and Huang, Xiaolong and Yang, Ziyi and Khademi, Mahmoud and Zhang, Dongdong and Awadalla, Hany Hassan and Fung, Yi R. and Chen, Weizhu and Cheng, Minhao and Wei, Furu},
  year = 2025,
  month = oct,
  number = {arXiv:2503.19551},
  eprint = {2503.19551},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.19551},
  urldate = {2025-11-12},
  abstract = {Large language models (LLMs) achieve strong performance across diverse tasks, driven by high-quality web data used in pre-training. However, recent studies indicate web data is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate scaling laws of synthetic data by introducing SYNTHLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our experiments with SYNTHLLM on math domain include: (1) SYNTHLLM generates synthetic data that reliably adheres to rectified scaling law across various model sizes; (2) Performance gains gradually diminish near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SYNTHLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to raw pre-training data, offering a viable path toward continued improvement in model performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Qin et al. - 2025 - Scaling Laws of Synthetic Data for Language Models.pdf}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@misc{raeScalingLanguageModels2022,
  title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and {d'Autume}, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  year = 2022,
  eprint = {2112.11446},
  archiveprefix = {arXiv}
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = 2020,
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67}
}

@misc{raffelExploringLimitsTransfer2023,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = 2023,
  month = sep,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.10683},
  urldate = {2024-11-29},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf}
}

@article{raschkaIntroductionLatestTechniques2021,
  title = {An Introduction to the Latest Techniques},
  author = {Raschka, Sebastian},
  year = 2021,
  pages = {52}
}

@book{raschkaMachineLearningPyTorch2022,
  title = {Machine Learning with {{PyTorch}} and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  year = 2022,
  publisher = {Packt Publishing},
  address = {Birmingham}
}

@misc{raschkaModelEvaluationModel2020,
  title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
  author = {Raschka, Sebastian},
  year = 2020,
  eprint = {1811.12808},
  urldate = {2023-01-16},
  archiveprefix = {arXiv}
}

@article{raschkaRecentTrendsTechnologies2021,
  title = {Recent Trends, Technologies, and Challenges},
  author = {Raschka, Sebastian},
  year = 2021,
  pages = {54}
}

@misc{RecipeTrainingNeural,
  title = {A Recipe for Training Neural Networks},
  urldate = {2021-12-03}
}

@article{ribeiroEnsembleApproachBased2020,
  title = {Ensemble Approach Based on Bagging, Boosting and Stacking for Short-Term Prediction in Agribusiness Time Series},
  author = {Ribeiro, Matheus Henrique Dal Molin and {dos Santos Coelho}, Leandro},
  year = 2020,
  journal = {Applied Soft Computing},
  volume = {86},
  pages = {105837},
  doi = {10.1016/j.asoc.2019.105837}
}

@article{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = 2020,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  doi = {10.1162/tacl\_a\_00349},
  urldate = {2023-06-17}
}

@misc{ronenMachineLearningTrade2022,
  title = {Machine Learning and Trade Direction Classification: Insights from the Corporate Bond Market},
  author = {Fedenia, Mark A. and Ronen, Tavy and Nam, Seunghan},
  year = 2022,
  eprint = {ssrn.4213313},
  doi = {10.2139/ssrn.4213313},
  archiveprefix = {SSRN}
}

@article{rosenthalModelingTradeDirection2012,
  title = {Modeling Trade Direction},
  author = {Rosenthal, Dale W. R.},
  year = 2012,
  journal = {Journal of Financial Econometrics},
  volume = {10},
  number = {2},
  pages = {390--415},
  doi = {10.1093/jjfinec/nbr014}
}

@article{rossiMachineLearning,
  title = {Predicting Stock Market Returns with Machine Learning},
  author = {Rossi, Alberto},
  year = 2018,
  journal = {Unpublished Working Paper},
  pages = {44}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for Natural Language Processing},
  author = {Rothman, Denis},
  year = 2021,
  publisher = {Packt Publishing},
  address = {Birmingham}
}

@misc{rozemberczkiShapleyValueMachine2022,
  title = {The Shapley Value in Machine Learning},
  author = {Rozemberczki, Benedek and Watson, Lauren and Bayer, P{\'e}ter and Yang, Hao-Tsung and Kiss, Oliv{\'e}r and Nilsson, Sebastian and Sarkar, Rik},
  year = 2022,
  eprint = {2202.05594},
  urldate = {2022-10-27},
  archiveprefix = {arXiv}
}

@misc{rubachevRevisitingPretrainingObjectives2022,
  title = {Revisiting Pretraining Objectives for Tabular Deep Learning},
  author = {Rubachev, Ivan and Alekberov, Artem and Gorishniy, Yury and Babenko, Artem},
  year = 2022,
  eprint = {2207.03208},
  archiveprefix = {arXiv}
}

@article{rubinBayesianBootstrap1981,
  title = {The Bayesian Bootstrap},
  author = {Rubin, Donald B.},
  year = 1981,
  journal = {The Annals of Statistics},
  volume = {9},
  number = {1},
  pages = {130--134},
  doi = {10.1214/aos/1176345338},
  urldate = {2023-06-30}
}

@article{rubinInferenceMissingData1976,
  title = {Inference and Missing Data},
  author = {Rubin, Donald B.},
  year = 1976,
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  doi = {10.1093/biomet/63.3.581},
  urldate = {2022-11-28}
}

@article{rubinsteinRelationBinomialTrinomial2000,
  title = {On the Relation between Binomial and Trinomial Option Pricing Models},
  author = {Rubinstein, Mark},
  year = 2000,
  journal = {The Journal of Derivatives},
  volume = {8},
  number = {2},
  pages = {47--50},
  doi = {10.3905/jod.2000.319149},
  urldate = {2021-07-04}
}

@article{sametFoundationsMultidimensionalMetric,
  title = {Foundations of Multidimensional and Metric Data Structures},
  author = {Samet, Hanan},
  pages = {1022}
}

@book{sandersSequentialParallelAlgorithms2019,
  title = {Sequential and Parallel Algorithms and Data Structures: The Basic Toolbox},
  author = {Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year = 2019,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-25209-0},
  urldate = {2020-10-31}
}

@article{sarkarMarketSidednessInsights2023,
  title = {Market Sidedness: Insights into Motives for Trade Initiation},
  author = {Sarkar, Asani and Schwartz, Robert A},
  year = 2023
}

@article{savickasInferringDirectionOption2003,
  title = {On Inferring the Direction of Option Trades},
  author = {Savickas, Robert and Wilson, Arthur J},
  year = 2003,
  journal = {Journal of Financial and Quantitative Analysis},
  volume = {38},
  pages = {881--902},
  doi = {10.2307/4126747}
}

@article{schaferRecommenderSystemsEcommerce1999,
  title = {Recommender Systems in {{E-commerce}}},
  author = {Schafer, J Ben and Konstan, Joseph and Riedl, John},
  year = 1999,
  pages = {9},
  doi = {10.1145/336992.337035}
}

@article{schapireBoostingMarginNew1998,
  title = {Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods},
  author = {Schapire, Robert E. and Bartlett, Peter and Freund, Yoav and Lee, Wee Sun},
  year = 1998,
  journal = {The Annals of Statistics},
  volume = {26},
  number = {5},
  doi = {10.1214/aos/1024691352}
}

@article{schapireStrengthWeakLearnability1990,
  title = {The Strength of Weak Learnability},
  author = {Schapire, Robert E.},
  year = 1990,
  journal = {Machine Learning},
  volume = {5},
  number = {2},
  pages = {197--227},
  doi = {10.1007/BF00116037},
  urldate = {2022-12-14}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A}} Unified Embedding for Face Recognition and Clustering},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = 2015,
  pages = {815--823},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298682},
  urldate = {2023-06-26}
}

@techreport{securitiesandexchangecommissionReportConcerningExaminations2007,
  title = {Report Concerning Examinations of Options Order Routing and Execution},
  author = {{Securities and Exchange Commission}},
  year = 2007,
  institution = {{Securities and Exchange Commission}},
  urldate = {2023-06-26}
}

@article{shahriariTakingHumanOut2016,
  title = {Taking the Human out of the Loop: A Review of Bayesian Optimization},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = 2016,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  doi = {10.1109/JPROC.2015.2494218}
}

@article{shallueMeasuringEffectsData,
  title = {Measuring the Effects of Data Parallelism on Neural Network Training},
  author = {Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and {Sohl-Dickstein}, Jascha and Frostig, Roy and Dahl, George E}
}

@article{shaniEvaluatingRecommendationSystems,
  title = {Evaluating Recommendation Systems},
  author = {Shani, Guy and Gunawardana, Asela},
  pages = {43},
  doi = {10.1007/978-0-387-85820-3\_8}
}

@incollection{shapleyValueNpersonGames1953,
  title = {A Value for N-Person Games},
  booktitle = {Contributions to the {{Theory}} of {{Games}} ({{AM-28}}), {{Volume II}}},
  author = {Shapley, L. S.},
  editor = {Kuhn, Harold William and Tucker, Albert William},
  year = 1953,
  pages = {307--318},
  publisher = {Princeton University Press},
  doi = {10.1515/9781400881970-018}
}

@inproceedings{shavittRegularizationLearningNetworks2018,
  title = {Regularization Learning Networks: Deep Learning for Tabular Datasets},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Shavitt, Ira and Segal, Eran},
  year = 2018,
  pages = {11},
  address = {Montr\'eal}
}

@misc{shazeerAdafactorAdaptiveLearning2018,
  title = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author = {Shazeer, Noam and Stern, Mitchell},
  year = 2018,
  eprint = {1804.04235},
  urldate = {2023-01-19},
  archiveprefix = {arXiv}
}

@misc{shazeerGLUVariantsImprove2020,
  title = {{{GLU}} Variants Improve Transformer},
  author = {Shazeer, Noam},
  year = 2020,
  eprint = {2002.05202},
  archiveprefix = {arXiv}
}

@article{shiQuantizedTrainingGradient,
  title = {Quantized Training of Gradient Boosting Decision Trees},
  author = {Shi, Yu and Ke, Guolin and Chen, Zhuoming and Zheng, Shuxin and Liu, Tie-Yan}
}

@article{shumwayDelistingBiasCRSP1997,
  title = {The Delisting Bias in {{CRSP}} Data},
  author = {Shumway, Tyler},
  year = 1997,
  journal = {The Journal of Finance},
  volume = {52},
  number = {1},
  pages = {327--340},
  doi = {10.1111/j.1540-6261.1997.tb03818.x},
  urldate = {2021-10-29}
}

@misc{shwartz-zivTabularDataDeep2021,
  title = {Tabular Data: Deep Learning Is Not All You Need},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = 2021,
  eprint = {2106.03253},
  urldate = {2022-10-05},
  archiveprefix = {arXiv}
}

@inproceedings{smiejaProcessingMissingData2018,
  title = {Processing of Missing Data by Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{\'S}mieja, Marek and Struski, {\L}ukasz and Tabor, Jacek and Zieli{\'n}ski, Bartosz and Spurek, Przemys{\l}aw},
  year = 2018,
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-11-28}
}

@misc{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = 2017,
  eprint = {1706.03825},
  doi = {10.48550/arXiv.1706.03825},
  urldate = {2022-12-17},
  archiveprefix = {arXiv}
}

@misc{smithAperiodicMonotile2023,
  title = {An Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = 2023,
  eprint = {2303.10798},
  urldate = {2023-05-15},
  archiveprefix = {arXiv}
}

@misc{smithCyclicalLearningRates2017,
  title = {Cyclical Learning Rates for Training Neural Networks},
  author = {Smith, Leslie N.},
  year = 2017,
  eprint = {1506.01186},
  archiveprefix = {arXiv}
}

@misc{smithDonDecayLearning2018,
  title = {Don't Decay the Learning Rate, Increase the Batch Size},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = 2018,
  eprint = {1711.00489},
  urldate = {2022-12-20},
  archiveprefix = {arXiv}
}

@misc{smithSuperConvergenceVeryFast2018,
  title = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle = {Super-{{Convergence}}},
  author = {Smith, Leslie N. and Topin, Nicholay},
  year = 2018,
  month = may,
  number = {arXiv:1708.07120},
  eprint = {1708.07120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.07120},
  urldate = {2024-11-24},
  abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Smith and Topin - 2018 - Super-Convergence Very Fast Training of Neural Networks Using Large Learning Rates.pdf;/Users/markusbilz/Zotero/storage/2CIE4ML2/1708.html}
}

@misc{SmolDoclingUltracompactVisionlanguage,
  title = {{{SmolDocling}}: {{An}} Ultra-Compact Vision-Language Model for End-to-End Multi-Modal Document Conversion},
  urldate = {2025-04-13},
  howpublished = {https://arxiv.org/html/2503.11576v1},
  file = {/Users/markusbilz/Zotero/storage/T7GA77ES/2503.html}
}

@inproceedings{snoekPracticalBayesianOptimization2012,
  title = {Practical Bayesian Optimization of Machine Learning Algorithms},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  year = 2012,
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-11-01}
}

@misc{somepalliSaintImprovedNeural2021,
  title = {{{SAINT}}: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C. Bayan and Goldstein, Tom},
  year = 2021,
  eprint = {2106.01342},
  urldate = {2022-10-04},
  archiveprefix = {arXiv}
}

@inproceedings{songAutoIntAutomaticFeature2019,
  title = {{{AutoInt}}: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  year = 2019,
  eprint = {1810.11921},
  pages = {1161--1170},
  doi = {10.1145/3357384.3357925},
  archiveprefix = {arXiv}
}

@misc{SparseAutoencodersUsing2020,
  title = {Sparse Autoencoders Using L1 Regularization with {{PyTorch}}},
  year = 2020,
  urldate = {2021-11-15}
}

@article{srivastavaDropoutSimpleWay,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = 2014,
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958}
}

@misc{statquestwithjoshstarmerGradientBoostPart2019,
  title = {Gradient Boost Part 2 (of 4): Regression Details},
  author = {{StatQuest with Josh Starmer}},
  year = 2019,
  urldate = {2021-12-25}
}

@inproceedings{steckCosineSimilarityEmbeddingsReally2024,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  year = 2024,
  month = may,
  eprint = {2403.05440},
  primaryclass = {cs},
  pages = {887--890},
  doi = {10.1145/3589335.3651526},
  urldate = {2024-11-28},
  abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosinesimilarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Steck et al. - 2024 - Is Cosine-Similarity of Embeddings Really About Similarity.pdf}
}

@book{stewartCalculusEarlyTranscendentals2016,
  title = {Calculus: Early Transcendentals},
  author = {Stewart, James},
  year = 2016,
  edition = {Eighth edition},
  publisher = {Cengage Learning},
  address = {Boston, MA, USA}
}

@book{strangIntroductionLinearAlgebra2016,
  title = {Introduction to Linear Algebra},
  author = {Strang, Gilbert},
  year = 2016,
  edition = {5th edition},
  publisher = {Cambridge press},
  address = {Wellesley}
}

@article{strangLinearAlgebraLearning,
  title = {Linear {{Algebra}} and {{Learning}} from {{Data}}},
  author = {Strang, Gilbert},
  langid = {english},
  file = {/Users/markusbilz/Zotero/storage/D9QQFMJ5/Strang - Linear Algebra and Learning from Data.pdf}
}

@article{stroblConditionalVariableImportance2008,
  title = {Conditional Variable Importance for Random Forests},
  author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
  year = 2008,
  journal = {BMC Bioinformatics},
  volume = {9},
  number = {1},
  pages = {307},
  doi = {10.1186/1471-2105-9-307},
  urldate = {2023-04-11}
}

@article{stutzUnderstandingImprovingRobustness,
  title = {Understanding and Improving Robustness and Uncertainty Estimation in Deep Learning},
  author = {Stutz, David},
  pages = {74}
}

@misc{sukhbaatarAugmentingSelfattentionPersistent2019,
  title = {Augmenting Self-Attention with Persistent Memory},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  year = 2019,
  eprint = {1907.01470},
  archiveprefix = {arXiv}
}

@incollection{sunAdaBoostLSTMEnsembleLearning2018,
  title = {{{AdaBoost-LSTM}} Ensemble Learning for Financial Time Series Forecasting},
  booktitle = {Computational {{Science}} -- {{ICCS}} 2018},
  author = {Sun, Shaolong and Wei, Yunjie and Wang, Shouyang},
  editor = {Shi, Yong and Fu, Haohuan and Tian, Yingjie and Krzhizhanovskaya, Valeria V. and Lees, Michael Harold and Dongarra, Jack and Sloot, Peter M. A.},
  year = 2018,
  volume = {10862},
  pages = {590--597},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93713-7\_55}
}

@misc{suRoFormerEnhancedTransformer2022,
  title = {{{RoFormer}}: Enhanced Transformer with Rotary Position Embedding},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = 2022,
  eprint = {2104.09864},
  urldate = {2023-04-14},
  archiveprefix = {arXiv}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = 2014,
  volume = {27},
  pages = {3104--3112},
  publisher = {MIT Press},
  address = {Montreal, QC, Canada}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the Inception Architecture for Computer Vision},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  year = 2016,
  pages = {2818--2826},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.308}
}

@misc{takaseLayerNormalizationsResidual2022,
  title = {On Layer Normalizations and Residual Connections in Transformers},
  author = {Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun},
  year = 2022,
  eprint = {2206.00330},
  urldate = {2023-01-19},
  archiveprefix = {arXiv}
}

@inproceedings{tangAnalysisAttentionMechanisms2018,
  title = {An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation},
  booktitle = {Proceedings of the {{Third Conference}} on {{Machine Translation}}: {{Research Papers}}},
  author = {Tang, Gongbo and Sennrich, Rico and Nivre, Joakim},
  year = 2018,
  pages = {26--35},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/W18-6304},
  urldate = {2023-01-04}
}

@article{tanhaSemisupervisedSelftrainingDecision2017,
  title = {Semi-Supervised Self-Training for Decision Tree Classifiers},
  author = {Tanha, Jafar and {van Someren}, Maarten and Afsarmanesh, Hamideh},
  year = 2017,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {8},
  number = {1},
  pages = {355--370},
  doi = {10.1007/s13042-015-0328-7}
}

@misc{tayEfficientTransformersSurvey2022,
  title = {Efficient Transformers: A Survey},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = 2022,
  eprint = {2009.06732},
  archiveprefix = {arXiv}
}

@misc{taylorGalacticaLargeLanguage2022,
  title = {Galactica: A Large Language Model for Science},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year = 2022,
  eprint = {2211.09085},
  urldate = {2023-05-03},
  archiveprefix = {arXiv}
}

@inproceedings{tenneyBERTRediscoversClassical2019,
  title = {{{BERT}} Rediscovers the Classical {{NLP}} Pipeline},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = 2019,
  pages = {4593--4601},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1452}
}

@article{theissenTestAccuracyLee2001,
  title = {A Test of the Accuracy of the {{Lee}}/{{Ready}} Trade Classification Algorithm},
  author = {Theissen, Erik},
  year = 2001,
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {11},
  number = {2},
  pages = {147--165},
  doi = {10.1016/s1042-4431(00)00048-2}
}

@misc{thoppilanLaMDALanguageModels2022,
  title = {{{LaMDA}}: Language Models for Dialog Applications},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and {Meier-Hellstern}, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and {Hoffman-John}, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and {Aguera-Arcas}, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  year = 2022,
  eprint = {2201.08239},
  urldate = {2023-01-31},
  archiveprefix = {arXiv}
}

@misc{tianVisualAutoregressiveModeling2024,
  title = {Visual {{Autoregressive Modeling}}: {{Scalable Image Generation}} via {{Next-Scale Prediction}}},
  shorttitle = {Visual {{Autoregressive Modeling}}},
  author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  year = 2024,
  month = jun,
  number = {arXiv:2404.02905},
  eprint = {2404.02905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.02905},
  urldate = {2025-04-05},
  abstract = {We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine ``next-scale prediction'' or ``next-resolution prediction'', diverging from the standard raster-scan ``next-token prediction''. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256\texttimes 256 benchmark, VAR significantly improve AR baseline by improving Fr\'echet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with 20\texttimes{} faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Tian et al. - 2024 - Visual Autoregressive Modeling Scalable Image Generation via Next-Scale Prediction.pdf}
}

@article{tobekDoesSourceFundamental,
  title = {Does the Source of Fundamental Data Matter?},
  author = {Tobek, Ondrej and Hronec, Martin},
  pages = {78},
  doi = {10.2139/ssrn.3150654}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: Open and Efficient Foundation Language Models},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = 2023,
  eprint = {2302.13971},
  urldate = {2023-04-29},
  archiveprefix = {arXiv}
}

@misc{TransformerArchitecturePositional,
  title = {Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog},
  urldate = {2021-12-28}
}

@misc{TransformersLucasBeyer,
  title = {Transformers with Lucas Beyer, Google Brain - {{YouTube}}},
  urldate = {2022-10-27}
}

@article{tsaiPredictingStockReturns2011,
  title = {Predicting Stock Returns by Classifier Ensembles},
  author = {Tsai, Chih-Fong and Lin, Yuah-Chiao and Yen, David C. and Chen, Yan-Min},
  year = 2011,
  journal = {Applied Soft Computing},
  volume = {11},
  number = {2},
  pages = {2452--2459},
  doi = {10.1016/j.asoc.2010.10.001}
}

@misc{tuningplaybookgithub,
  title = {Deep Learning Tuning Playbook},
  author = {Godbole, Varun and Dahl, George E. and Gilmer, Justin and Shallue, Christopher J. and Nado, Zachary},
  year = 2023,
  urldate = {2023-06-01}
}

@article{tunstallNaturalLanguageProcessing2022,
  title = {Natural Language Processing with Transformers},
  author = {Tunstall, Lewis},
  year = 2022,
  pages = {409}
}

@misc{turnerBayesianOptimizationSuperior2021,
  title = {Bayesian Optimization Is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020},
  author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  year = 2021,
  eprint = {2104.10201},
  archiveprefix = {arXiv}
}

@article{twalaGoodMethodsCoping2008,
  title = {Good Methods for Coping with Missing Data in Decision Trees},
  author = {Twala, B.E.T.H. and Jones, M.C. and Hand, D.J.},
  year = 2008,
  journal = {Pattern Recognition Letters},
  volume = {29},
  number = {7},
  pages = {950--956},
  doi = {10.1016/j.patrec.2008.01.010}
}

@inproceedings{ucarSubTabSubsettingFeatures2021,
  title = {{{SubTab}}: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
  year = 2021,
  volume = {34},
  pages = {18853--18865},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-27}
}

@article{universityoftechnologysydneyaustraliaLiquidityMotivatedAlgorithm2008,
  title = {A Liquidity Motivated Algorithm for Discerning Trade Direction},
  author = {{University of Technology Sydney, Australia} and Hasan, David and Prather, Laurie and {Bond University, Australia}},
  year = 2008,
  journal = {Multinational Finance Journal},
  volume = {12},
  number = {1/2},
  pages = {45--66},
  doi = {10.17578/12-1/2-3},
  urldate = {2023-04-05}
}

@article{vanbreugelCanYouRely,
  title = {Can {{You Rely}} on {{Your Model Evaluation}}? {{Improving Model Evaluation}} with {{Synthetic Test Data}}},
  author = {{van Breugel}, Boris and Seedat, Nabeel and Imrie, Fergus},
  abstract = {Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines---including real test data alone---in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing approaches. Overall, these results raise the question of whether we need a paradigm shift away from limited real test data towards synthetic test data.},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/van Breugel et al. - Can You Rely on Your Model Evaluation Improving Model Evaluation with Synthetic Test Data.pdf}
}

@article{vanbreugelSyntheticDataReal,
  title = {Synthetic {{Data}}, {{Real Errors}}: {{How}} ({{Not}}) to {{Publish}} and {{Use Synthetic Data}}},
  author = {{van Breugel}, Boris and Qian, Zhaozhi},
  abstract = {Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach---using synthetic data as if it is real---leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE)---a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-density regions of the original data, for which the generative uncertainty is largest.},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/van Breugel and Qian - Synthetic Data, Real Errors How (Not) to Publish and Use Synthetic Data.pdf}
}

@article{vandermaatenVisualizingDataUsing2008,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = 2008,
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605}
}

@article{vanengelenSurveySemisupervisedLearning2020,
  title = {A Survey on Semi-Supervised Learning},
  author = {{van Engelen}, Jesper E. and Hoos, Holger H.},
  year = 2020,
  journal = {Machine Learning},
  volume = {109},
  number = {2},
  pages = {373--440},
  doi = {10.1007/s10994-019-05855-6}
}

@misc{vasuImprovedOneMillisecond2022,
  title = {An Improved One Millisecond Mobile Backbone},
  author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
  year = 2022,
  eprint = {2206.04040},
  urldate = {2022-07-28},
  archiveprefix = {arXiv}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = 2017,
  series = {{{NeurIPS}} 2017},
  volume = {30},
  pages = {6000--6010},
  publisher = {Curran Associates, Inc.},
  address = {Long Beach, CA}
}

@inproceedings{verscheldeGPUAccelerationNewton2014,
  title = {{{GPU}} Acceleration of Newton's Method for Large Systems of Polynomial Equations in Double Double and Quad Double Arithmetic},
  booktitle = {2014 {{IEEE Intl Conf}} on {{High Performance Computing}} and {{Communications}}, 2014 {{IEEE}} 6th {{Intl Symp}} on {{Cyberspace Safety}} and {{Security}}, 2014 {{IEEE}} 11th {{Intl Conf}} on {{Embedded Software}} and {{Syst}} ({{HPCC}},{{CSS}},{{ICESS}})},
  author = {Verschelde, Jan and Yu, Xiangcheng},
  year = 2014,
  pages = {161--164},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/HPCC.2014.31},
  urldate = {2021-04-18}
}

@inproceedings{vigInvestigatingGenderBias2020,
  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = 2020,
  volume = {33},
  pages = {12388--12401},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-01-29}
}

@article{Vijh_1990,
  title = {Liquidity of the {{CBOE}} Equity Options},
  author = {Vijh, Anand M.},
  year = 1990,
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1990.tb02431.x},
  mag_id = {1979846182},
  pmcid = {null},
  pmid = {null}
}

@article{vijhStockClosingPrice2020,
  title = {Stock Closing Price Prediction Using Machine Learning Techniques},
  author = {Vijh, Mehar and Chandola, Deeksha and Tikkiwal, Vinay Anand and Kumar, Arun},
  year = 2020,
  journal = {Procedia Computer Science},
  volume = {167},
  pages = {599--606},
  doi = {10.1016/j.procs.2020.03.326},
  urldate = {2022-07-12}
}

@inproceedings{voitaAnalyzingMultiHeadSelfAttention2019,
  title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  year = 2019,
  pages = {5797--5808},
  publisher = {Association for Computational Linguistics},
  address = {Florence},
  doi = {10.18653/v1/P19-1580}
}

@misc{wangAcceleratingPhi2CodeLlama,
  title = {Accelerating {{Phi-2}}, {{CodeLlama}}, {{Gemma}} and Other {{Gen AI}} Models with {{ONNX Runtime}}},
  author = {Wang, Sunghoon Choi, Yufeng Li, Kshama Pawar, Ashwini Khade, Ye, Parinita Rahi},
  urldate = {2024-11-26},
  abstract = {Improvements with ONNX Runtime for inferencing popular Gen AI models},
  howpublished = {https://onnxruntime.ai/blogs/accelerating-phi-2},
  langid = {english},
  file = {/Users/markusbilz/Zotero/storage/MLMIU78T/accelerating-phi-2.html}
}

@article{wangAttentionbasedTransactionalContext,
  title = {Attention-Based Transactional Context Embedding for next-Item Recommendation},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing and Huang, Xiaoshui and Lian, Defu and Liu, Wei},
  pages = {8},
  doi = {10.1609/aaai.v32i1.11851}
}

@article{wangForecastingMethodStock2020,
  title = {Forecasting Method of Stock Market Volatility in Time Series Data Based on Mixed Model of {{ARIMA}} and Xgboost},
  author = {Wang, Yan and Guo, Yuankai},
  year = 2020,
  journal = {China Communications},
  volume = {17},
  number = {3},
  pages = {205--221},
  doi = {10.23919/JCC.2020.03.017},
  urldate = {2022-07-12}
}

@misc{wangKernelFalconAutonomousGPU,
  title = {{{KernelFalcon}}: {{Autonomous GPU Kernel Generation}} via {{Deep Agents}} -- {{PyTorch}}},
  shorttitle = {{{KernelFalcon}}},
  author = {Wang, Laura and at Meta, the PyTorch Team},
  urldate = {2025-11-12},
  langid = {american},
  file = {/Users/markusbilz/Zotero/storage/ZZS6EMHE/kernelfalcon-autonomous-gpu-kernel-generation-via-deep-agents.html}
}

@inproceedings{wangLearningDeepTransformer2019,
  title = {Learning Deep Transformer Models for Machine Translation},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F. and Chao, Lidia S.},
  year = 2019,
  pages = {1810--1822},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1176}
}

@misc{wangLinformerSelfattentionLinear2020,
  title = {Linformer: Self-Attention with Linear Complexity},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = 2020,
  eprint = {2006.04768},
  urldate = {2022-12-10},
  archiveprefix = {arXiv}
}

@misc{wangNonuniformNegativeSampling2021,
  title = {Nonuniform {{Negative Sampling}} and {{Log Odds Correction}} with {{Rare Events Data}}},
  author = {Wang, HaiYing and Zhang, Aonan and Wang, Chong},
  year = 2021,
  month = oct,
  number = {arXiv:2110.13048},
  eprint = {2110.13048},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.13048},
  urldate = {2025-11-13},
  abstract = {We investigate the issue of parameter estimation with nonuniform negative sampling for imbalanced data. We first prove that, with imbalanced data, the available information about unknown parameters is only tied to the relatively small number of positive instances, which justifies the usage of negative sampling. However, if the negative instances are subsampled to the same level of the positive cases, there is information loss. To maintain more information, we derive the asymptotic distribution of a general inverse probability weighted (IPW) estimator and obtain the optimal sampling probability that minimizes its variance. To further improve the estimation efficiency over the IPW method, we propose a likelihood-based estimator by correcting log odds for the sampled data and prove that the improved estimator has the smallest asymptotic variance among a large class of estimators. It is also more robust to pilot misspecification. We validate our approach on simulated data as well as a real click-through rate dataset with more than 0.3 trillion instances, collected over a period of a month. Both theoretical and empirical results demonstrate the effectiveness of our method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Wang et al. - 2021 - Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data.pdf}
}

@incollection{wangPerceivingNextChoice2017,
  title = {Perceiving the next Choice with Comprehensive Transaction Embeddings for Online Recommendation},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing},
  editor = {Ceci, Michelangelo and Hollm{\'e}n, Jaakko and Todorovski, Ljup{\v c}o and Vens, Celine and D{\v z}eroski, Sa{\v s}o},
  year = 2017,
  volume = {10535},
  pages = {285--302},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-71246-8\_18},
  urldate = {2021-05-04}
}

@misc{wangSurveyDataSynthesis2024,
  title = {A {{Survey}} on {{Data Synthesis}} and {{Augmentation}} for {{Large Language Models}}},
  author = {Wang, Ke and Zhu, Jiahui and Ren, Minjie and Liu, Zeming and Li, Shiwei and Zhang, Zongye and Zhang, Chenkai and Wu, Xiaoyu and Zhan, Qiqi and Liu, Qingjie and Wang, Yunhong},
  year = 2024,
  month = oct,
  number = {arXiv:2410.12896},
  eprint = {2410.12896},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.12896},
  urldate = {2024-11-26},
  abstract = {The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Wang et al. - 2024 - A Survey on Data Synthesis and Augmentation for Large Language Models.pdf}
}

@misc{wangSurveySessionbasedRecommender2020,
  title = {A Survey on Session-Based Recommender Systems},
  author = {Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet and Lian, Defu},
  year = 2020,
  eprint = {1902.04864},
  urldate = {2021-04-22},
  archiveprefix = {arXiv}
}

@article{wangTransTabLearningTransferable,
  title = {{{TransTab}}: Learning Transferable Tabular Transformers across Tables},
  author = {Wang, Zifeng and Sun, Jimeng}
}

@misc{wangWizMapScalableInteractive2023,
  title = {{{WizMap}}: {{Scalable Interactive Visualization}} for {{Exploring Large Machine Learning Embeddings}}},
  author = {Wang, Zijie J. and Hohman, Fred and Chau, Duen Horng},
  year = 2023,
  eprint = {2306.09328},
  urldate = {2023-06-26},
  archiveprefix = {arXiv}
}

@article{waszczukAssemblingInternationalEquity2014,
  title = {Assembling International Equity Datasets -- Review of Studies on the Cross-Section of Returns},
  author = {Waszczuk, Antonina},
  year = 2014,
  journal = {Procedia Economics and Finance},
  volume = {15},
  pages = {1603--1612},
  doi = {10.1016/S2212-5671(14)00631-5},
  urldate = {2021-10-29}
}

@misc{WaybackMachine2023,
  title = {Wayback {{Machine}}},
  year = 2023,
  month = mar,
  urldate = {2024-11-28},
  howpublished = {https://web.archive.org/web/20230323002555/https://tnq177.github.io/data/transformers\_without\_tears.pdf},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/2023 - Wayback Machine.pdf}
}

@misc{weiTheoreticalAnalysisSelfTraining2022,
  title = {Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
  author = {Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
  year = 2022,
  eprint = {2010.03622},
  urldate = {2023-03-31},
  archiveprefix = {arXiv}
}

@article{welchComprehensiveLookEmpirical2008,
  title = {A Comprehensive Look at the Empirical Performance of Equity Premium Prediction},
  author = {Welch, Ivo and Goyal, Amit},
  year = 2008,
  journal = {Review of Financial Studies},
  volume = {21},
  number = {4},
  pages = {1455--1508},
  doi = {10.1093/rfs/hhm014}
}

@misc{wengLearningNotEnough2021,
  title = {Learning with Not Enough Data Part 1: Semi-Supervised Learning},
  author = {Weng, Lilian},
  year = 2021,
  urldate = {2022-10-13},
  chapter = {posts}
}

@misc{WhenMachinesTrade,
  title = {When Machines Trade on Corporate Disclosures: Using Text Analytics for Investment Strategies \textbar{} Elsevier Enhanced Reader},
  doi = {10.1016/j.dss.2022.113892},
  urldate = {2023-05-01}
}

@inproceedings{wiegreffeAttentionNotNot2019,
  title = {Attention Is Not Not Explanation},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  year = 2019,
  pages = {11--20},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1002}
}

@book{wilkeFundamentalsDataVisualization2019,
  title = {Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures},
  shorttitle = {Fundamentals of Data Visualization},
  author = {Wilke, Claus},
  year = 2019,
  edition = {First edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {Cover -- Copyright -- Table of Contents -- Preface -- Chapter 1. Introduction -- Part I. From Data to Visualization -- Chapter 2. Visualizing Data: Mapping Data onto Aesthetics -- Chapter 3. Coordinate Systems and Axes -- Chapter 4. Color Scales -- Chapter 5. Directory of Visualizations -- Chapter 6. Visualizing Amounts -- Chapter 7. Visualizing Distributions: Histograms and Density Plots -- Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots -- Chapter 9. Visualizing Many Distributions at Once -- Chapter 10. Visualizing Proportions -- Chapter 11. Visualizing Nested Proportions -- Chapter 12. Visualizing Associations Among Two or More Quantitative Variables -- Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable -- Chapter 14. Visualizing Trends -- Chapter 15. Visualizing Geospatial Data -- Chapter 16. Visualizing Uncertainty -- Part II. Principles of Figure Design -- Chapter 17. The Principle of Proportional Ink -- Chapter 18. Handling Overlapping Points -- Chapter 19. Common Pitfalls of Color Use -- Chapter 20. Redundant Coding -- Chapter 21. Multipanel Figures -- Chapter 22. Titles, Captions, and Tables -- Chapter 23. Balance the Data and the Context -- Chapter 24. Use Larger Axis Labels -- Chapter 25. Avoid Line Drawings -- Chapter 26. Don't Go 3D -- Part III. Miscellaneous Topics -- Chapter 27. Understanding the Most Commonly Used Image File Formats -- Chapter 28. Choosing the Right Visualization Software -- Chapter 29. Telling a Story and Making a Point -- Annotated Bibliography -- Technical Notes -- References -- Index -- About the Author -- Colophon},
  isbn = {978-1-4920-3108-6 978-1-4920-3103-1},
  langid = {english},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Wilke - 2019 - Fundamentals of data visualization a primer on making informative and compelling figures.epub}
}

@misc{wuMemorizingTransformers2022,
  title = {Memorizing Transformers},
  author = {Wu, Yuhuai and Rabe, Markus N. and Hutchins, DeLesley and Szegedy, Christian},
  year = 2022,
  eprint = {2203.08913},
  urldate = {2023-01-17},
  archiveprefix = {arXiv}
}

@inproceedings{xiongLayerNormalizationTransformer2020,
  title = {On Layer Normalization in the Transformer Architecture},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  year = 2020,
  volume = {21},
  pages = {10524--10533},
  publisher = {PMLR},
  address = {Online}
}

@article{yangStockPricePrediction2021,
  title = {Stock Price Prediction Based on Xgboost and {{LightGBM}}},
  author = {Yang, Yue and Wu, Yang and Wang, Peikun and Jiali, Xu},
  editor = {Wen, F. and Ziaei, S.M.},
  year = 2021,
  journal = {E3S Web of Conferences},
  volume = {275},
  pages = {01040},
  doi = {10.1051/e3sconf/202127501040},
  urldate = {2022-07-12}
}

@inproceedings{yanMachineLearningStock2007,
  title = {Machine Learning for Stock Selection},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '07},
  author = {Yan, Robert J. and Ling, Charles X.},
  year = 2007,
  pages = {1038},
  publisher = {ACM Press},
  address = {San Jose, California, USA},
  doi = {10.1145/1281192.1281307},
  urldate = {2021-11-03}
}

@misc{yaoZeroQuantEfficientAffordable2022,
  title = {{{ZeroQuant}}: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author = {Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  year = 2022,
  eprint = {2206.01861},
  doi = {10.48550/arXiv.2206.01861},
  urldate = {2022-11-23},
  archiveprefix = {arXiv}
}

@inproceedings{yarowskyUnsupervisedWordSense1995,
  title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Yarowsky, David},
  year = 1995,
  pages = {189--196},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, MA},
  doi = {10.3115/981658.981684}
}

@inproceedings{yinTaBERTPretrainingJoint2020,
  title = {{{TaBERT}}: Pretraining for Joint Understanding of Textual and Tabular Data},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  year = 2020,
  pages = {8413--8426},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.745},
  urldate = {2023-01-24},
  file = {/Users/markusbilz/Library/CloudStorage/OneDrive-Personal/Documents/05 - Wissen & Weiterbildung/paper/Yin et al. - 2020 - TaBERT pretraining for joint understanding of textual and tabular data.pdf}
}

@article{yiWhyNotUse2020,
  title = {Why Not to Use Zero Imputation? {{Correcting}} Sparsity Bias in Training Neural Networks},
  author = {Yi, Joonyoung and Lee, Juhyuk and Kim, Kwang Joon and Hwang, Sung Ju and Yang, Eunho},
  year = 2020,
  pages = {27}
}

@inproceedings{yoonVIMEExtendingSuccess2020,
  title = {Vime: Extending the Success of Self- and Semi-Supervised Learning to Tabular Domain},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and {van der Schaar}, Mihaela},
  year = 2020,
  series = {{{NeurIPS}} 2020},
  volume = {33},
  pages = {11033--11043},
  publisher = {Curran Associates, Inc.},
  address = {Red Hook, NY}
}

@misc{zengAreTransformersEffective2022,
  title = {Are Transformers Effective for Time Series Forecasting?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  year = 2022,
  eprint = {2205.13504},
  urldate = {2022-12-24},
  archiveprefix = {arXiv}
}

@article{zhaiDirectBoostingApproach,
  title = {A Direct Boosting Approach for Semi-Supervised Classification},
  author = {Zhai, Shaodan and Xia, Tian and Li, Zhongliang and Wang, Shaojun}
}

@misc{zhangDiveDeepLearning2021,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year = 2021
}

@article{zhangUptodateComparisonStateoftheart2017,
  title = {An Up-to-Date Comparison of State-of-the-Art Classification Algorithms},
  author = {Zhang, Chongsheng and Liu, Changchang and Zhang, Xiangliang and Almpanidis, George},
  year = 2017,
  journal = {Expert Systems with Applications},
  volume = {82},
  pages = {128--150},
  doi = {10.1016/j.eswa.2017.04.003},
  urldate = {2022-07-11}
}

@article{zhengFeatureEngineeringMachine,
  title = {Feature Engineering for Machine Learning},
  author = {Zheng, Alice and Casari, Amanda},
  pages = {217}
}

@misc{zhuClusteringStructureMicrostructure2021,
  title = {Clustering Structure of Microstructure Measures},
  author = {Zhu, Liao and Sun, Ningning and Wells, Martin T.},
  year = 2021,
  eprint = {2107.02283},
  archiveprefix = {arXiv}
}

@article{zhuSemiSupervisedLearningLiterature,
  title = {Semi-Supervised Learning Literature Survey},
  author = {Zhu, Xiaojin},
  pages = {60}
}

@inproceedings{zophRethinkingPretrainingSelftraining2020,
  title = {Rethinking Pre-Training and Self-Training},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin Dogus and Le, Quoc},
  year = 2020,
  volume = {33},
  pages = {3833--3845},
  publisher = {Curran Associates, Inc.}
}

@misc{zouStockMarketPrediction2022,
  title = {Stock Market Prediction via Deep Learning Techniques: A Survey},
  author = {Zou, Jinan and Zhao, Qingying and Jiao, Yang and Cao, Haiyao and Liu, Yanxi and Yan, Qingsen and Abbasnejad, Ehsan and Liu, Lingqiao and Shi, Javen Qinfeng},
  year = 2022,
  eprint = {2212.12717},
  urldate = {2022-12-29},
  archiveprefix = {arXiv}
}
